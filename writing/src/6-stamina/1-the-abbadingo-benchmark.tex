\section{The Abbadingo benchmark\label{section:stamina-abbadingo}}

Abbadingo is designed as a grid of 16 induction problems \cite{Lang:1998}. A competing learner gets a set of training strings labeled as positive or negative by an hidden DFA. It is required to predict the labels that the DFA would assign to a set of unlabeled testing strings. The 16 problems are classified in a grid along two dimensions of difficulty. 
\begin{itemize}
\item The first is the size of the target automata, as measured by the number of states (64, 128, 256 or 512). 
\item The second is the sparsity of the training sample, in terms of four sparsity levels. These levels have been tuned by manually inspecting the learning curves of the Trakhtenbrot-Barzdin algorithm \cite{Trakhtenbrot:1973, Lang:1992}. This algorithm was a state-of-the-art induction algorithm in 1997; the problem grid has been adjusted in such a way that it solves the four problems with the largest samples.
\end{itemize}

The target automata, training sets and test strings of Abbadingo were all drawn from uniform random distributions.
\begin{itemize}
\item Random automata were generated by constructing and minimizing directed graphs of binary degree. Edge and state labels were chosen by flipping a fair coin. 
\item A training set for a target automaton $A$ were made of a random sample drawn without replacement from a uniform distribution over the collection of all binary strings. The average length of those strings were chosen in order to have a good chance of reaching the deepest state of the automaton, a necessary criteria toward a structurally complete sample (see Section~\ref{subsection:gi-background-search-space}). 
\item A testing set were composed of 1800 strings drawn from the remaining strings. Training and test sets did not overlap.
\end{itemize}

The testing protocol of Abbadingo runs as follows. The learner labels each string of the test set and submits this labeling to an online oracle\footnote{available at http://abbadingo.cs.nuim.ie/}. To avoid hill climbing, where the feedback of the competition server could be used by the learner to iteratively optimize a first solution, this oracle only provides a single bit of feedback. The latter tells whether or not the problem is broken. 

A problem is considered broken if the accuracy of the labeling reached at least 99\%. The accuracy is computed as the proportion of the 1800 test strings correctly labeled. During the competition itself, the first participant to break a given problem gained credit for it. Abbadingo allowed multiple winners by defining a partial order on problem difficulty: a problem $A$ is considered harder than a problem $B$ if its DFA has more states \emph{and} its training sample is sparser. 

Two winners, Rodney Price and Hugues Julli\'e, won the competition with similar algorithms relying on what has since been called \emph{evidence-driven state merging} (EDSM). When performing generalization of a sample through state merging (see Chapter~\ref{chapter:inductive-synthesis}), this term captures the strategy of first performing state merges that are supported by the most evidence, according to a specific scoring heuristics. Among other contributions, the competition helped finding the Blue-fringe heuristics to refine which state pairs to merge preferably (see Section~\ref{BlueFringe}).

\subsection{Limitations for evaluating inductive model synthesis}

After the formal competition, Abbadingo evolved into a reusable protocol and online benchmark for induction techniques. The evaluations in Chapter~\ref{chapter:inductive-synthesis} were conducted on a similar protocol. However, Abbadingo sets the size of the alphabet to two symbols only. This limits the relevance of reusing its protocol for evaluating behavior model synthesis techniques. Behavior models are commonly defined on larger sets of events. For instance, the small phone case study studied in the previous chapter uses 16 distinct events for a state machine of only 23 states. Moreover, the automata and samples generated with Abbadingo are notable different from those commonly found in software engineering problems:

\begin{description}
\item[Automata] The automata randomly generated by Abbadingo have a small, quasi-constant, state degree (in number of incident edges). Being defined on a binary alphabet, a canonical automaton of $n$ states has at least $n$ and at most $2n$ edges; these edges are uniformly distributed over all states. 

Automata modeling software systems involve transitions that may be triggered by any of a large number of events. The latter may capture mouse clicks, function names, IO events, etc. The number of outgoing transitions for a given state can be very large and vary significantly from state to state. In the process of designing Stamina, a review of state machines found in the literature has shown the following~\cite{Walkinshaw:2008}: 
\begin{itemize}
\item most states have in- and out-degrees of one or two transitions,
\item a few states have a high in-degree, e.g. those modeling exception handling,
\item a few states have a high out-degree, e.g. an \emph{idle} state where a software agent waits for external stimuli in terms of input events,
\item a few states are \emph{sink} accepting states, that is, they have an out-degree of 0, e.g., explicitly modeling the ability of a system to halt.
\end{itemize}

The automata generated with Abbadingo do not fit such characteristics, due to the small alphabet size. Moreover, because of the uniform edge distribution implied, the protocol would not naturally lead to automata presenting aforementioned characteristics, even if adapted for handling larger alphabets.

\item[Samples] Abbadingo was inspired from the PAC framework for generating samples \cite{Valiant:1984}. In this setting the distribution of the observed strings is external to the target machine. Samples are therefore simply drawn from uniform random distribution over the collection of binary strings up to a some prescribed length. The target machine is only used to classify them as positive or negative. The respective number of positive and negative strings is naturally balanced.

This procedure is no longer adequate when working with larger alphabets and state machines presenting the characteristics previously described. Because of the small average out-degree of states, an overwhelming majority of random sequences from $\Sigma^*$ would be negative strings. Moreover, the few positive strings that would be made available would probably not provide a sufficient coverage of the target machine for obtaining good induction results (see Section~\ref{section:inductive-background}).

For behavior models, a \emph{generative} procedure seems more adequate, where the target machine itself is used to generate the learning sample. This is representative of the way such samples are obtained in the software engineering research: end-users scenarios come from a mental model of the software that is, a variant of the target machine~\cite{Combefis:2011}; execution traces are sometimes obtained through software testing and monitoring; and so on.

\item[Scoring] The accuracy measure is defined in Abbadingo as the proportion of test strings that are correctly classified by the learned automaton. This choice is questionable if one relaxes the assumption of balanced test samples~\cite{Walkinshaw:2008}. In the extreme case of a test set largely overwhelmed by negative strings, for example, a learner classifying all strings as negative would still obtain a comfortable score. A better measure should consider the acceptance of positive strings and the rejection of negative ones as equally important. 
\end{description}

Conducting sound evaluations requires rethinking important parts of the underlying protocol. To capitalize over and share the cost of such work, we designed Stamina as both a public complement and an alternative to Abbadingo. Focusing on characteristics of behavior models, it also acts as a call to crossfertilization between the machine learning and software engineering communities. To achieve this, Stamina first took the form of an online induction competition before evolving into an online benchmark, in a similar spirit to Abbadingo. The next section details the Stamina setup as designed for the competition. Changes made to the platform since the end of the competition will be explained in Section~\ref{section:stamina-platform}.
