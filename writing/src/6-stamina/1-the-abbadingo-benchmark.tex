\section{The Abbadingo benchmark\label{section:stamina-abbadingo}}

Abbadingo is designed as a grid of 16 induction problems. A competing learner is provided a set of training strings labeled as positive or negative by an unseen DFA and is required to predict the labels that the DFA would assign to a set of testing strings. The 16 problems are classified in a grid of two difficulty dimensions. The first one is the size of the target automata, as the number of its states (64, 128, 256 or 512). The second is the sparsity of the training sample, in terms of four sparsity levels. These levels have been tuned by manually inspecting learning curves of the Trakhenbrot-Barzdin algorithm. Indeed, the latter was a state-of-the-art induction algorithm in 1997. Therefore, the problem grid has been adjusted in such a way that it solves the four problems with largest samples.

Target automata, training and test strings of Abbadingo have all been drawn from uniform random distributions. Random automata have been generated by constructing and minimizing degree-2 directed graphs (as a consequence of using a binary alphabet), edges and states labels (final or not) having been chosen by flipping a fair coin. To keep the generation of the training set sufficiently simple, only automata of depth of exactly $2log_2(n)-2$ have been kept for inclusion in the problem grid. A training set for a target of size $n$ is made of a random sample drawn without replacement from a uniform distribution over the collection of $16n^2 -1$ binary strings whose length lies between 0 and $2log_2(n)+3$ inclusively. This latter bound has been chosen to have a good chance of reaching the deepest state of the automaton, a necessary criteria toward a structurally complete sample, which is not guaranteed though. A testing set consists in 1800 strings drawn from the remaining strings. As a consequence, training and test sets do not overlap.

The testing protocol of Abbadingo consists in the learner labeling each string of the test set and submitting this labeling to an online oracle\footnote{available at http://abbadingo.cs.nuim.ie/}. To avoid hill climbing -- where the feedback of the competition server could be used by the learner to iteratively optimize a first solution -- this oracle only provides a single bit of feedback which tells whether or not the problem is broken. A problem is considered broken if the accuracy (computed as the proportion of the 1800 test strings correctly labeled), of the labeling is at least 99\%. During the competition itself, the first participant to break a given problem gained credit for it. Abbadingo allowed multiple winners by defining a partial order on problem difficulty: a problem $A$ is considered harder than a problem $B$ if its DFA has more states \emph{and} its training sample is sparser. 

Two winners, Rodney Price and Hugues Julli\'e, won the competition with similar algorithms relying on what has since been called \emph{evidence driven state merging} (EDSM). This term captures the strategy of first performing state merges that are supported by the most evidence, according to a specific scoring heuristics. Among other contributions, the competition has helped finding a good scoring heuristics based on the number of final states merged during the determinization process. Mixing this evidence driven idea with the red-blue merge order described previously (see chapter~\ref{chapter:inductive-synthesis}) leads to the particularly fast and simple algorithm known as \emph{Blue-fringe}, for which the post-analysis Abbadingo paper provides a reference description~\cite{Lang:1998}.

\subsection{Weaknesses for evaluating inductive model synthesis}

Since the end of the competition, Abbadingo has evolved to become a reusable protocol and online benchmark for induction techniques. To situate and compare our results, the evaluations proposed in chapter~\ref{chapter:inductive-synthesis} of the present thesis have been precisely conducted on a similar protocol. However, the fact that Abbadingo fixes the size of the alphabet to only two letters limits the relevance of reusing its protocol for evaluating behavior model synthesis techniques. This is mainly because behavior models are commonly defined on larger sets of events. As an example, the small phone case-study studied in the previous chapter already uses 16 distinct events for a state machine of only 23 states. Strictly speaking, the weakness is not the small alphabet size itself but has to be found in the impact of using binary alphabets on the different artifacts required for building such a benchmark:

\begin{description}
\item[Automata] The automata randomly generated by Abbadingo have a small, quasi-constant, state degree (number of incident edges). This is a consequence of using binary alphabets: due to the its deterministic nature, an automaton of $n$ states has at least $n$ and at most $2n$ edges; these edges are uniformly distributed over all states. 

In contrast, automata modeling software systems involve transitions that may be triggered by any of a large number of events (mouse clicks, function names IO events, etc.). The number of outgoing transitions for a given state can be very large and vary significantly from state to state. A review of the state machines found in the literature shows that while most states have in- and out-degrees of one or two transitions, state machines tend to contain a small proportion of states with a high in-degree (e.g. those modeling exception handling) or a high out-degree (an \emph{idle} state where a software agent waits for external stimuli in terms of input events). Other states are \emph{sink} accepting states, that is, they have an out-degree of 0 (explicit modeling of the ability of a system to halt)~\cite{Walkinshaw:2008}.

The automata generated with the Abbadingo procedure do not present such characteristics, due to the small alphabet size. Moreover, because of the uniform edge distribution it implies, this procedure would not naturally lead to automata presenting the characteristics aforementioned even if it was adapted for handling larger alphabets.

\item[Samples] Abbadingo samples are simply drawn from uniform random distribution over the collection of all (binary) strings up to a prescribed length. The target machine is used to classify them as positive or negative. For statistical reasons, the respective number of positive and negative strings is naturally balanced.

Unfortunately, this procedure is no longer viable when working with larger alphabets (unless state machines present similar characteristics to the ones of Abbadingo, which is not the case, as previously discussed). Indeed, in this case an overwhelming majority of random sequences in $\Sigma^*$ are negative. Moreover, the few positive strings that would be made available would probably not provide a sufficient coverage of the target machine for obtaining good induction results (see Section~\ref{section:inductive-background}).

\item[Scoring] The choice of the accuracy measure (defined in Abbadingo as the proportion of test strings that are correctly classified by the automaton learned) is also arguable if one relaxes the assumption of having balanced test samples~\cite{Walkinshaw:2008}. In the extreme case of a test set largely overwhelmed by negative strings for example, a learner classifying all strings as negative would still obtain a comfortable score. A better measure should consider the acceptance of positive strings and the rejection of negative ones equally important. 
\end{description}

As shown, conducting sound evaluations without making the assumption of a binary alphabet requires rethinking important parts of the underlying protocol. To capitalize over and share the cost of such a work we designed Stamina as both a public complement and an alternative to Abbadingo. Focusing on characteristics of behavior models, it is also a call to cross-fertilization between the machine learning and software engineering communities. To achieve this goal in a fun way, Stamina first took the form of an online induction competition before evolving to an online benchmark, in a similar spirit to Abbadingo. The next section details the Stamina setup as designed for the competition. Changes made to the platform since the end of the competition will be explained in Section~\ref{section:stamina-platform}.

