\section{The Abbadingo benchmark\label{section:stamina-abbadingo}}

Abbadingo is designed as a grid of 16 induction problems \cite{Lang:1998}. A competing learner is provided a set of training strings labeled as positive or negative by an unseen DFA and is required to predict the labels that the DFA would assign to a set of previously unseen testing strings. The 16 problems are classified in a grid of two difficulty dimensions. The first one is the size of the target automata, as measured by the number of its states (64, 128, 256 or 512). The second is the sparsity of the training sample, in terms of four sparsity levels. These levels have been tuned by manually inspecting the learning curves of the Trakhtenbrot-Barzdin algorithm \cite{Trakhtenbrot:1973, Lang:1992}. This algorithm was a state-of-the-art induction algorithm in 1997; the problem grid has been adjusted in such a way that it solves the four problems with the largest samples.

Target automata, training and test strings of Abbadingo have all been drawn from uniform random distributions. Random automata have been generated by constructing and minimizing degree-2 directed graphs, edges and states labels having been chosen by flipping a fair coin. A training set for a target automaton $A$ is made of a random sample drawn without replacement from a uniform distribution over the collection of all binary strings whose length lies between 0 and $depth(A)+5$ inclusively\footnote{the depth of an automaton is defined as the length of the the longest shortest path from the initial state to any other state.}. This latter bound has been chosen to have a good chance of reaching the deepest state of the automaton, a necessary criteria toward a structurally complete sample (see Section~\ref{subsection:gi-background-search-space}). A testing set consists in 1800 strings drawn from the remaining strings. Training and test sets do not overlap.

The testing protocol of Abbadingo consists in the learner labeling each string of the test set and submitting this labeling to an online oracle\footnote{available at http://abbadingo.cs.nuim.ie/}. To avoid hill climbing, where the feedback of the competition server could be used by the learner to iteratively optimize a first solution, this oracle only provides a single bit of feedback. The latter tells whether or not the problem is broken. 

A problem is considered broken if the accuracy of the labeling is at least 99\%. The accuracy is computed as the proportion of the 1800 test strings correctly labeled. During the competition itself, the first participant to break a given problem gained credit for it. Abbadingo allowed multiple winners by defining a partial order on problem difficulty: a problem $A$ is considered harder than a problem $B$ if its DFA has more states \emph{and} its training sample is sparser. 

Two winners, Rodney Price and Hugues Julli\'e, won the competition with similar algorithms relying on what has since been called \emph{evidence driven state merging} (EDSM). When performing generalization of a sample through state merging (see Chapter~\ref{chapter:inductive-synthesis}), this term captures the strategy of first performing state merges that are supported by the most evidence, according to a specific scoring heuristics. Among other contributions, the competition helped finding the Blue-fringe heuristics to refine which state pairs to merge preferably (see Section~\ref{BlueFringe}).

\subsection{Weaknesses for evaluating inductive model synthesis}

After the formal competition, Abbadingo evolved to a reusable protocol and online benchmark for induction techniques. The evaluations in chapter~\ref{chapter:inductive-synthesis} have been conducted on a similar protocol. However, Abbadingo fixes the size of the alphabet to two letters only, that limits the relevance of reusing its protocol for evaluating behavior model synthesis techniques. Behavior models are commonly defined on larger sets of events. For instance, the small phone case-study studied in the previous chapter uses 16 distinct events for a state machine of only 23 states. Moreover, important differences exist between automata and samples generated with Abbadingo and those commonly observed in the software engineering community:

\begin{description}
\item[Automata] The automata randomly generated by Abbadingo have a small, quasi-constant, state degree (number of incident edges). As being defined on a binary alphabet, a canonical automaton of $n$ states has at least $n$ and at most $2n$ edges; these edges are uniformly distributed over all states. 

Automata modeling software systems involve transitions that may be triggered by any of a large number of events. The latter model mouse clicks, function names IO events, etc. The number of outgoing transitions for a given state can be very large and vary significantly from state to state. A review of the state machines found in the literature shows that~\cite{Walkinshaw:2008}: 
\begin{itemize}
\item most states have in- and out-degrees of one or two transitions,
\item a few states have a high in-degree, e.g. those modeling exception handling
\item a few states have a high out-degree, e.g. an \emph{idle} state where a software agent waits for external stimuli in terms of input events,
\item a few states are \emph{sink} accepting states, taht is, they have an out-degree of 0, e.g. the explicit modeling of the ability of a system to halt.
\end{itemize}

The automata generated with the Abbadingo procedure do not present such characteristics, due to the small alphabet size. Moreover, because of the uniform edge distribution it implies, this procedure would not naturally lead to automata presenting the characteristics aforementioned even if it was adapted for handling larger alphabets.

\item[Samples] Abbadingo inspires from the PAC framework for generating samples \cite{Valiant:1984}. In such a setting the distribution of the observed strings is external to the target machine. Samples are therefore simply drawn from uniform random distribution over the collection of all (binary) strings up to a prescribed length. The target machine is only used to classify them as positive or negative. The respective number of positive and negative strings is naturally balanced.

This procedure is no longer adequate when working with larger alphabets and state machines presenting the characteristics previously described. Because of the small average out-degree of states, an overwhelming majority of random sequences in $\Sigma^*$ would be negative strings. Moreover, the few positive strings that would be made available would probably not provide a sufficient coverage of the target machine for obtaining good induction results (see Section~\ref{section:inductive-background}).

With behavior models a \emph{generative} procedure seems more adequate, where the target machine itself is used to generate the learning sample. This is representative of the way such samples are obtained in the software engineering community: end-users scenarios come from a mental model of the software, that is, a variant of the target machine; execution traces are sometimes obtained through software testing and monitoring; and so on.

\item[Scoring] The choice of the accuracy measure (defined in Abbadingo as the proportion of test strings that are correctly classified by the automaton learned) is also arguable if one relaxes the assumption of having balanced test samples~\cite{Walkinshaw:2008}. In the extreme case of a test set largely overwhelmed by negative strings for example, a learner classifying all strings as negative would still obtain a comfortable score. A better measure should consider the acceptance of positive strings and the rejection of negative ones equally important. 
\end{description}

Conducting sound evaluations requires rethinking important parts of the underlying protocol. To capitalize over and share the cost of such a work we designed Stamina as both a public complement and an alternative to Abbadingo. Focusing on characteristics of behavior models, it is also a call to cross-fertilization between the machine learning and software engineering communities. To achieve this goal in a fun way, Stamina first took the form of an online induction competition before evolving to an online benchmark, in a similar spirit to Abbadingo. The next section details the Stamina setup as designed for the competition. Changes made to the platform since the end of the competition will be explained in Section~\ref{section:stamina-platform}.
