\chapter{Towards an Evaluation Platform for Inductive Model Synthesis\label{chapter:stamina}}

Grammar induction algorithms are commonly evaluated by reporting accuracy measures on the learned model for increasing sizes of the training sample. For example, plots in the previous chapter illustrated the convergence of QSM and ASM towards a ``good'' model when their input becomes rich enough. 

This kind of setup can also be used to compare various induction techniques by evaluating them on common benchmarks of increasing difficulty. Along this line, the Abbadingo contest had a notable impact~\cite{Lang:1998}. A grid of grammar induction challenges unbroken at that time were proposed online. The competition resulted in the discovery of the Blue-Fringe heuristics used by QSM for selecting state pairs to merge (see Section~\ref{BlueFringe}). Since then, Abbadingo has been frequently used as a reusable benchmark and evaluation protocol for grammar induction, e.g., \cite{Lucas:2003, Bongard:2005, Lucas:2005, Adriaans:2006, Dupont:2008, Lambeau:2008, Heule:2010}.

A serious weakness of the Abbadingo benchmark is its restriction to induction problems for automata defined on alphabets of two symbols only. Freezing certain parameters, such as the alphabet size, is necessary for keeping a competition accessible to participants. As a side effect, it limits the generalization of the conclusions and the reusability of the competition protocol. 

This effect has been observed when conducting the evaluations reported in the thesis. On one side, Abbadingo allowed our results to be compared on a sound and agreed benchmarking protocol. On the other side, limiting the evaluations to binary alphabets lacks credibility from a software engineering standpoint. Behavior models are commonly defined on thirty events or more. One might thus reasonably question the representativeness of synthetic machines used in the previous chapter for capturing behavior models.

Extending the Abbadingo protocol itself to take larger alphabets into account appears inappropriate. By design, the generation of learning samples in Abbadingo is independent from the target automaton. This feature inspires from \emph{probably approximately correct} (PAC) learning, a sound theoretical framework from machine learning \cite{Valiant:1984}. However, this independence assumption between that target automaton and the learning sample does not accurately reflect the way samples are obtained from end-user scenarios or automated testing of distributed systems. In particular, generating strings with a distribution independent of the state machine would lead to an overwhelming amount of negative scenarios for typical behavior models. Abbadingo is further discussed in Section~\ref{section:stamina-abbadingo}.

For that reason, an alternative benchmark is proposed here, called Sta-mina (for \emph{State Machine Inference Approaches}). Stamina inspires from Abbadingo but focuses on the complexity of the learning with respect to the alphabet size. Our protocol relies on adapted procedures for generating automata and samples. 

Stamina has initially taken the form of an induction competition, in a similar spirit to Abbadingo. The competition has officially ran between March and December 2010 and was organized in collaboration with the universities of Sheffield and Leicester; it has been officially sponsored by the Engineering and Physical Sciences Research Council\footnote{http://www.epsrc.ac.uk/}. The detailed setup of Stamina is explained in section~\ref{section:stamina-setup}. 

The winning algorithm, DFASAT, as well as a few additional competitors have significantly outperformed the Blue-fringe baseline on a variety of problems. DFASAT mixes SAT solving and state merging and uses a new scoring heuristics that proved especially useful to infer the kind of state machines considered in the competition. Section~\ref{section:stamina-results} presents the main results of the competition and a brief description of the winning technique. 

Further to fostering efforts around the induction problem, the competition raised interest from at least three communities, namely, Machine Learning, Software Engineering and Formal Methods. The Stamina website\footnote{http://stamina.chefbe.net/} is still accessible; it has been updated to serve as an online benchmark and evaluation platform for model synthesis, as discussed in Section~\ref{section:stamina-platform}.

\input{src/6-stamina/1-the-abbadingo-benchmark}
\input{src/6-stamina/2-stamina-setup}
\input{src/6-stamina/3-competition-results}
\input{src/6-stamina/4-evolution}

\section*{Summary}

This chapter discussed \emph{Stamina}, an evaluation platform for inductive model synthesis. Stamina follows a previous platform from the machine learning community known as Abbadingo. Our work on Stamina sets a basis for overcoming limitations of Abbadingo for evaluating inductive model synthesis techniques such as QSM and ASM. In particular, random automata in Stamina are more representative of the state machines found in software engineering problems.

Stamina was first designed to be a formal competition in automaton induction before evolving into an online benchmark. The competition was won by DFASAT, a novel induction algorithm that significantly pushed forward the state-of-the-art by outperforming Blue-fringe used as baseline. The design of Stamina make the competition results especially relevant for our software engineering standpoint. In particular, DFASAT opens important perspectives for inductive model synthesis in our application domain.
