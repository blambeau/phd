\chapter{Towards an evaluation platform for inductive model synthesis\label{chapter:stamina}}

As discussed in chapter~\ref{chapter:evaluation}, an induction algorithm is commonly evaluated by reporting an accuracy measure of the model it learns for increasing size of a training sample. Plots typically illustrates the convergence towards a ``good'' model when the training sample becomes rich enough. Practical evaluations have been conducted on RPNI as well as numerous of its variants in \cite{Lang98,Damas06,Dupont08,Lambeau08}. In that respect, the Abbadingo contest~\cite{Lang98} have had a notable impact, first because it provides a reusable evaluation protocol and benchmarks for induction algorithms and second, because it helped discovering the evidence-driven state merging heuristic used by Blue-Fringe, on which QSM itself relies. 

Abbadingo focussed on learning DFAs from positive and negative strings along two difficulty dimensions, the size of the automaton to learn and the sparsity of the training sample. All other parameters were fixed, notably the alphabet size which was equal to 2 for all problems (other candidate parameters include the ratio of final states over non-final ones, the density and depth of the automaton, the ratio of positive over negative strings in the training sample, the distribution of string lengths, and so on).

Fixing parameters of benchmarks and contests helps keeping competition objectives sufficiently clear and understandable for users. If also makes sound analysis results easier to obtain, due to the presence of only a few degrees of freedom. On the other hand, fixing parameter values has an important impact because it limits the applicability of the conclusions drawn as well as their generalization. In that respect, the conclusions we drawn in the previous chapter all rely on an -- sometimes implicit -- assumption of an alphabet of 2 letters. While extrapolating results to larger alphabets appears natural, it must be done with care.

Our use of the Abbadingo protocol was initially motivated by the necessity to compare our results with previous ones of grammatical inference. Unfortunately, restricting our attention to binary alphabets is arguable from a software engineering point of view, for two reasons. First because behavior models are often defined on 30 events or more. Second, and probably more important, because using binary alphabets has a strong influence on the class of automata considered. Therefore, one can certainly question the representativeness of synthetic machines and samples used in the previous chapter for mimicing behavior models. 

This overal questioning of the influence of the alphabet size on induction performance motivated the Stamina competition (Stamina stands for \emph{State Machine Inference Approaches}) that we officially ran between march and december 2010. Its aim was to identify the best technique to infer state machines presenting characteristics of behavior models while focussing on the complexity of the learning with respect to the alphabet size. For this, Stamina extends Abbadingo -- with which it shares certain features -- but relies on adapted protocols for generating target machines and samples, among other differences. As presented later in this chapter, the competition helped identifying a new technique -- formally known as DFASAT -- for learning automata defined on large alphabets. The technique mixes SAT solving and state merging and, as an interresting side effect, identifies a new scoring heuristic that proved useful to tackle the learning task in presence of alphabets of more than two letters. The Stamina website\footnote{hosted at http://stamina.chefbe.net/} is still available and has been slightly updated to serve as an online benchmark and evaluation platform for model synthesis, instead of a formal competition.

This chapter presents this evaluation platform and the Stamina competition in more details. An overview of the competition is presented in section~\ref{subsection_stamina_overview}. Details about the competition setup -- state machines and samples, their generation and links with software engineering models, motivation of the chosen evaluation protocol and scoring metrics -- are  given in~\ref{subsection_stamina_setup}. The performance of RPNI and Blue-Fringe on the competition problems are detailed in ~\ref{subsection_stamina_baseline}. Last, participation and main competition results are presented in~\ref{subsection_stamina_results}.

\section{From Abbadingo to Stamina}

\subsection{Background on Abbadingo}

The Abbadingo competition was designed as a grid of 16 induction problems. The principle is straightforward: the learner is provided a set of training strings labeled as positive or negative by an unseen DFA and is required to predict the labels that the DFA would assign to a set of testing strings. The 16 problems formed a grid of two difficulty dimensions. The first dimension is the size of the target automata, as the number of its states (64, 128, 256 and 512 were considered). The second dimension is the sparsity of the training sample. Four sparsity levels were considered, with the exact number of strings -- a few thousands -- increasing also with the automaton size considered. The exact size of each training sample has been tuned by manually inspecting learning curves of the Trakhenbrot-Barzdin algorithm, considered as one of the state-of-the-art induction algorithm in 1997. The problem grid was adjusted in such a way that the latter algorithm solved the four problems with largest samples.

In Abbadingo, the target automata, training and test strings were all drawn from uniform random distributions. Random automata were generated by constructing and minimizing degree-2 directed graphs (for recall, only alphabet of two letters were considered), with the label of edges (the letter) and states (final or not) being choosen by flipping a fair coin. To keep the generation of the training set sufficiently simple, only automata of depth of exactly $2log_2(n)-2$ were considered for the problem grid. A training set for a target of size $n$ consisted of a random sample drawn without replacement from a uniform distribution over the collection of $16n^2 -1$ binary strings whose length lies between 0 and $2log_2(n)+3$ inclusively. This latter bound was chosen to have a good chance of reaching the deepest state of the automaton, a necessary criteria towards a structurally complete sample, which was not guaranteed though. The testing set consisted in 1800 strings drawn from the remaining strings and, as a consequence, does not overlap with the training set.

The testing protocol of Abbadingo consisted in the learner labeling each string of the test set and submitting these labelings to a testing oracle available online\footnote{the server is now hosted at http://abbadingo.cs.nuim.ie/}. To avoid hill climbing, this oracle only provided a 1 bit of feedback which told whether or not the accuracy (the ratio of the number of strings correctly labeled over the total number of strings, that is 1800) of the labeling was at least 99\%. In that case, the problem was considered solved and the participant gained credit for it provided that she was the first to break it. Abbadingo actually allowed multiple winners by defining a partial order of problem difficulty, namely, that a problem $A$ was harder than a problem $B$ if its DFA had more states \emph{and} its training sample was sparser. 

Two winners, Rodney Price and Hugues Julli\'e, won the competition with similar algorithms relying on what has since been called \emph{evidence driven state merging} (EDSM). This term captures the strategy of first performing state merges that are supported by the most evidence and, among other contributions, the competition helped finding a good scoring heuristics based on the number of final states merged. Also, mixing this evidence driven idea with the red-blue merge order described previously (see chapter~\ref{chapter:inductive-synthesis}) leads to the particularly fast and simple algorithm known as \emph{Blue-fringe}, for which Abbadingo provided a reference description~\cite{Lang98}.

\subsection{Inadequacies for software engineering models}

As stated earlier, the fact that Abbadingo fixed the size of the alphabet to only two letters limits the relevance of reusing its protocol for evaluating behavior model synthesis techniques. This is mainly because behavior models are commonly defined on larger sets of events. As an example, the small phone case-study of section~\ref{section:evaluation-re} already uses 16 distinct events for a state machine of only 23 states. This difference in sizes of the alphabets considered is probably not sufficient in itself to question the relevance of Abbadingo's protocol here. More problematic however are the implicit consequences of considering binary alphabets only:

\begin{itemize}
\item The state degree of deterministic finite automata defined on binary alphabets ranges from 0 to 2.

\end{itemize}

\section{Competition overview\label{subsection_stamina_overview}}

The competition consists in attempting to solve cells of a grid of 100 induction problems. Each problem consists in learning a regular language from a sample of positive and negative strings randomly drawn from a target machine. Following the competition objectives discussed in the previous section, the problems vary in terms of their difficulty, related to:

\begin{itemize}
\item the size of the alphabet in the target machine
\item the sparsity of the sample, that is, the extent to which the sample covers the behaviour of the target machine
\end{itemize}

\begin{table}
\begin{center}
\begin{tabular}{c|c c c c}
&\multicolumn{4}{|c}{Sparsity}\\ 
\textbf{$|\Sigma|$} & \textbf{100\%} & \textbf{50\%} & \textbf{25\%} & \textbf{12.5\%}\\
\hline
\textbf{2}  & 1-5   & 6-10  & 11-15 & 16-20 \\
\textbf{5}  & 21-25 & 26-30 & 31-35 & 36-40 \\
\textbf{10} & 41-45 & 46-50 & 51-55 & 56-60 \\
\textbf{20} & 61-65 & 66-70 & 71-75 & 76-80 \\
\textbf{50} & 81-85 & 86-90 & 91-95 & 96-100\\
\end{tabular}
\end{center}
\caption{\label{stamina:table:problem-grid}Grid of 100 problems distributing the induction difficulty among two dimensions: sparsity of the learning sample and alphabet size.}
\end{table}

The competition grid is divided in cells of 5 problems each, where each cell corresponds to a particular combination of sparsity and alphabet size. Table~\ref{stamina:table:problem-grid} shows how problems are distributed in cells. Easier problems (with a smaller alphabet and a larger sample) are towards the upper-left of the table, and the harder problems (larger alphabet and smaller sample) are towards the bottom-right. The conduct of the competition can be described as follows:

\begin{itemize}
\item Before launching, a target machine is randomly generated for each problem. A learning sample made of positive and negative strings is also drawn by randomly walking the obtained machine. The machine and the sample are generated so as to fit specific criteria of the cell holding the corresponding problem. Last, a test sample is similarly generated whose aim is to evaluate and score participant submissions later in the competition.
\item The target machines are not disclosed to the participants, in contrast to learning and test samples that are made available for download on a competition server. However, unlike the learning sample, the test sample is distributed unlabeled. 
\item For a specific problem of her choice, a participant is expected to run an induction algorithm on the corresponding learning sample. The model learned doing so is then used to label each string of the test sample. Such a sequence of binary labels is submitted on the competition server, which scores the submission by comparing that sequence to known labels for test strings. 
\item For reasons detailed later, the competition used the balanced classification rate (BCR) as scoring measure. A problem is considered broken when the score obtained is greater or equal to 0.99. A cell is considered broken by a participant if she breaks the five problems it contains. 
\item The winner of the competition was the first technique to solve a hardest cell, among those solved in the competition. In order to formally evaluate the respective difficulty of the different cells, each of them is given a number of points (from 1 to 4) based on the average performance of the Blue-Fringe algorithm. The latter is indeed considered as representative of the state of the art and an implementation has hence been made available for download. 
\end{itemize}

\section{Competition setup\label{subsection_stamina_setup}}

\subsection{State Machines}

In order to generate state machines representative of the ones encountered in the software engineering community, a quick review of software models has been conducted. Observations have been made on a small sample (about 20 systems) of case-study models found in research publications. State machine models were analyzed in terms of their states, transitions, alphabet sizes, in-/out degree and depth. Although the sample is too small to form any authoritative conclusions, findings can be interpreted as being indicative. Following these observations, the target machines used in the competition have been generated using a variant of the Forest-Fire algorithm~\cite{Leskovec2007}. The algorithm has been tuned to present the following characteristics:

\begin{description}

\item[Number of states] To keep the competition setup simple enough -- by keeping two difficulty dimensions only, namely sample sparsity and alphabet size --, all state machines have approximately 50 states. Although somewhat larger than the conventional state machines identified in the Software Engineering literature, this is to ensure that any techniques submitted to this competition could scale to infer models for reasonably complex software systems. 

\item[Alphabet size] Alphabet sizes in the competition range from 2 to 50 symbols. For recall, previous competitions such as Abbadingo focussed on 2 letter alphabets. This lower bound has been kept to facilitate result comparisons. 50 symbols is representative of the upper bound found on software models extracted from the scientific litterature. 

\item[Accepting ratio] A roughly equal proportion of accepting and rejecting states has been chosen, a feature is shared with Abbadingo. While most software models, and LTS in particular, have all accepting states it has been decided to keep non accepting states as well. Doing so keeps the competition sufficiently closed to former regular induction setups, and avoids restricting it to the inference of prefix-closed regular languages in particular. As already discussed in chapter~\ref{chapter:inductive-synthesis}, infering prefix-closed languages looks an easier problem than general regular inference. Hence, a competition winner would be expected to perform equaly well, if not better, when infering machines with all accepting states.

\item[Degree distribution] While most states have in- and out-degrees of one or two transitions, state machines of software systems tend to contain a small proportion of states with a high in-degree (\emph{authority} states) or a high out-degree (\emph{hub} states). Some states are \emph{sink} accepting states, that is, they have an out-degree of 0. Hub and/or authority states, for example, typically capture specific \emph{idle} states where a software agent waits for external stimuli in terms of input events (with numerous )possible candidates). Sink states can be interpreted as explicit modeling of the ability of a system to halt, and so on. Those degree characteristics can be observed in state machines generated for the competition. It contrasts with target machines of Abbadingo where \emph{hub} or \emph{authority} states could not be present due to the small alphabet size, for example.

\item[Deterministic and minimal] Following common setup of regular inference experiments, the machines considered in the competition are both deterministic and minimal.

\end{description}

\subsection{Training and test samples}

Generating random training and testing samples for past competitions has been straightforward. The approach for competitions such as Abbadingo has been to simply generate random binary sequences up to a prescribed length, and to subsequently use the target model to classify them as ``accept'' or ``reject''. However, doing so with machines that have a larger alphabet is no longer viable; an overwhelming majority of random sequences in $\Sigma^{*}$ is likely to be classified as ``reject'', and of the few sequences that are accepted, it is unlikely that they would provide any useful coverage of the target machine. 

For this reason, training and test samples have been generated using a dedicated generation procedure. This procedure aims at simulating the way examples of system behaviour are usually obtained in the software engineering community (a collection of program traces at an implementation level or the generation of scenarios at a design level, for instance). Samples generated present the following characteristics:

\begin{description}

\item[Generated by the target] Positive strings have been generated by randomly walking the target machines. A dedicated algorithm generates positive strings by walking through the automaton from the initial state, randomly selecting outgoing transitions with a uniform distribution. When an accepting state v is reached, the generation ends with a probability of $1.0/(1 + 2*outdegree(v))$. This procedure simulates an `'end of string'' transition from state $v$ with half the probability of an existing transition. The length distribution of the strings generated is approximately centered on $5 + depth(automaton)$, a feature shared with the Abbadingo competition.

\item[Negative strings] Negative strings are generated by randomly perturbing strings generated from the target machine. Three kinds of edit operation are considered: substituting, inserting, or deleting a symbol. The editing position is randomly chosen according to a uniform distribution over the string length. Substitution and insertion also use a uniform distribution over the alphabet. The number of editing operations is chosen according to a Poisson distribution (with a mean of 3) and the three editing operations have an equal change of being selected. The randomly edited string is included in the negative sample provided it is indeed rejected by the target machine. Otherwise, it is simply discarded.

\item[Samples] The random walk algorithm and perturbation procedure serve as building blocks for the generation of training and test samples for each problem. Using them, a set of 20.000 strings has first been sampled from the target machine. This sample contains roughly equal number of positive and negative strings and usually contains duplicates. The distinct strings of the initial sample are then equally partitioned into two sets, taking care of respecting the positive and negative balance in each one. The first set is used to generate the test sample, the second one to generate the learning sample. The test sample contains 1,500 strings randomly drawn without replacement from the first set. The test set neither contains duplicates (to avoid favoring repeated strings in the scoring metric), nor intersects with the training set. The official training sets are sampled from the second set with different levels of sparsity (100\%, 50\%, 25\%, 12.5\%) and usually contain duplicates, as a consequence of the random walk generation from the target machine. As a consequence of this procedure, training and test sets do not intersect.

\item[Empirically adjusted] the different parameters above (20,000 strings initially generated, 1,500 strings maximum in the test set and the learning sample size) have all been empirically adjusted to ensure good induction results using Blue-Fringe on the simplest problems (alphabet of size 2 with a learning sample sparsity of 100\%) without breaking the cell.

\end{description}

\subsection{Submission and Scoring}

Solutions are submitted as a binary string to the competition server. For this, the competitor is expected to produce a binary sequence of labels where, for each test string, a 1 is added to the sequence if the string is considered to be accepted, and a 0 otherwise. To establish the accuracy, the sequence is compared to a reference string representing the correct classifications of the test set on the target model. The overlap between the two binary strings is measured with the \emph{Balanced Classification Rate (BCR)}. The Harmonic BCR measure is chosen because it places an equal emphasis on the accuracy of an inferred model in terms of its acceptance of positive sequences, as well as its rejection of sequences that should be rejected. It also does not require the test set to be balanced in terms of its positive / negative sequences~\cite{Walkinshaw2008}. Relaxing such assumptions is important here because the natural positive/negative balancing commonly induced by a binary alphabet disappears with larger alphabet sizes.

Harmonic BCR combines two factors. $Sensitivity$ is the proportion of positive matches that are predicted to be positive. So in terms of the sets true positives ($TP$) and false positives ($FP$), $Sensitivity=\frac{|TP|}{|TP \cup FN|}$. $Specificity$ is the proportion of true negatives that are predicted to be negative, so $Specificity=\frac{|TN|}{|TN \cup FP|}$. Harmonic BCR is the harmonic mean of the two: $$BCR=\frac{2*Sensitivity*Specificity}{Sensitivity + Specificity}$$. Note that conventionally, BCR is simply computed as the arithmetic mean of sensitivity and specificity, but the harmonic mean is preferred here because it favours balance between the two.

To avoid hill-climbing -- where the competition server could be used as an oracle to help the learner to interactively optimize a first solution --, the competition server does not respond to a submission with the exact BCR score that it has obtained. Instead, it gives a binary feedback according to whether the corresponding problem is considered broken or not. A problem is considered broken if the BCR value is greater than or equal to 0.99. A cell is considered broken if all of its 5 problems are broken. A section dedicated to each participant has been implemented on the website to provide visual feedback about the performance of their algorithm(s). In this section, problems and cells of a personal problem grid turn to green when broken.

\section{Performance of RPNI and Blue-Fringe\label{subsection_stamina_baseline}}



\section{Main results\label{subsection_stamina_results}}



