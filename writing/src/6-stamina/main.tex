\chapter{Towards an evaluation platform for inductive model synthesis\label{chapter:stamina}}

As shown previously, experimental evaluations of RPNI allow illustrating the practical convergence of this family of algorithms, typically by drawing graphs reporting an accuracy measure versus the sparsity of the learning sample. Such experiments have been conducted on RPNI -- as well as numerous variants -- for different automaton sizes in \cite{Lang98,Damas06,Dupont08,Lambeau08}. However, these evaluations have all been made on machines presenting alphabets of two letters only. In contrast, it is not rare to encounter state machines with more than 30 events in the software engineering litterature. Among others, it is this lack of formal knowledge and experimental results about the effect of the alpÄ¥abet size on the convergence of RPNI and therefore, the arguable relevance of interpreting available ones over our software engineering setup, that initiated the Stamina competition. More precisely the main objectives of the competition were:

\begin{itemize}
\item To further investigate the influence of the alphabet size on the DFA induction problem, in particular its effect on the performance and/or convergence of state of the art algorithms,
\item To trigger the discovery of new induction techniques that potentially out-perform state of the art algorithms on problems representative of what is encountered in the software engineering community and in particular when target state machines have large alphabets,
\item To provide a benchmarking framework inspired by previous competitions in grammar induction but also fitting specific criteria of the software engineering community
\end{itemize}

The competition has successfully ran between march and december 2010. Its main contributions are:

\begin{itemize}
\item It provides experimental results illustrating convergence profiles of RPNI and BlueFringe with respect to the alphabet size of the target machine. In particular, we confirm an expected effect: if convergence in the limit is not hurted, increasing the size of the alphabet requires a similar increasing of the learning sample to converge in practice,
\item The competition provides an evaluation protocol for regular inference that can be used as an alternative to the one of Abbadingo when dealing with alphabets of more than two letters. This protocol includes a procedure for randomly generating state machines mimicing the ones encountered in the software engineering litterature, a procedure for generating learning and test samples via a random walk of the generated state machine and a scoring metric which prove useful in presence of an unbalanced proportion of positive and negative strings in learning and test samples,
\item Last but not least, the competition confirms the effectiveness of a recent approach to the DFA induction problem mixing evidence-driven state merging and SAT solving~\cite{Heule10}.
\end{itemize}

The following sections present the competition and its results in more details. An overview of the competition is presented in section~\ref{subsection_stamina_overview}. Details about the competition setup -- state machines and samples, their generation and links with software engineering models, motivation of the chosen evaluation protocol and scoring metrics -- are  given in~\ref{subsection_stamina_setup}. The performance of RPNI and Blue-Fringe on the competition problems are detailed in ~\ref{subsection_stamina_baseline}. Last, participation and main competition results are presented in~\ref{subsection_stamina_results}.

\section{Competition overview\label{subsection_stamina_overview}}

The competition consists in attempting to solve cells of a grid of 100 induction problems. Each problem consists in learning a regular language from a sample of positive and negative strings randomly drawn from a target machine. Following the competition objectives discussed in the previous section, the problems vary in terms of their difficulty, related to:

\begin{itemize}
\item the size of the alphabet in the target machine
\item the sparsity of the sample, that is, the extent to which the sample covers the behaviour of the target machine
\end{itemize}

\begin{table}
\begin{center}
\begin{tabular}{c|c c c c}
&\multicolumn{4}{|c}{Sparsity}\\ 
\textbf{$|\Sigma|$} & \textbf{100\%} & \textbf{50\%} & \textbf{25\%} & \textbf{12.5\%}\\
\hline
\textbf{2}  & 1-5   & 6-10  & 11-15 & 16-20 \\
\textbf{5}  & 21-25 & 26-30 & 31-35 & 36-40 \\
\textbf{10} & 41-45 & 46-50 & 51-55 & 56-60 \\
\textbf{20} & 61-65 & 66-70 & 71-75 & 76-80 \\
\textbf{50} & 81-85 & 86-90 & 91-95 & 96-100\\
\end{tabular}
\end{center}
\caption{\label{stamina:table:problem-grid}Grid of 100 problems distributing the induction difficulty among two dimensions: sparsity of the learning sample and alphabet size.}
\end{table}

The competition grid is divided in cells of 5 problems each, where each cell corresponds to a particular combination of sparsity and alphabet size. Table~\ref{stamina:table:problem-grid} shows how problems are distributed in cells. Easier problems (with a smaller alphabet and a larger sample) are towards the upper-left of the table, and the harder problems (larger alphabet and smaller sample) are towards the bottom-right. The conduct of the competition can be described as follows:

\begin{itemize}
\item Before launching, a target machine is randomly generated for each problem. A learning sample made of positive and negative strings is also drawn by randomly walking the obtained machine. The machine and the sample are generated so as to fit specific criteria of the cell holding the corresponding problem. Last, a test sample is similarly generated whose aim is to evaluate and score participant submissions later in the competition.
\item The target machines are not disclosed to the participants, in contrast to learning and test samples that are made available for download on a competition server. However, unlike the learning sample, the test sample is distributed unlabeled. 
\item For a specific problem of her choice, a participant is expected to run an induction algorithm on the corresponding learning sample. The model learned doing so is then used to label each string of the test sample. Such a sequence of binary labels is submitted on the competition server, which scores the submission by comparing that sequence to known labels for test strings. 
\item For reasons detailed later, the competition used the balanced classification rate (BCR) as scoring measure. A problem is considered broken when the score obtained is greater or equal to 0.99. A cell is considered broken by a participant if she breaks the five problems it contains. 
\item The winner of the competition was the first technique to solve a hardest cell, among those solved in the competition. In order to formally evaluate the respective difficulty of the different cells, each of them is given a number of points (from 1 to 4) based on the average performance of the Blue-Fringe algorithm. The latter is indeed considered as representative of the state of the art and an implementation has hence been made available for download. 
\end{itemize}

\section{Competition setup\label{subsection_stamina_setup}}

\subsection{State Machines}

In order to generate state machines representative of the ones encountered in the software engineering community, a quick review of software models has been conducted. Observations have been made on a small sample (about 20 systems) of case-study models found in research publications. State machine models were analyzed in terms of their states, transitions, alphabet sizes, in-/out degree and depth. Although the sample is too small to form any authoritative conclusions, findings can be interpreted as being indicative. Following these observations, the target machines used in the competition have been generated using a variant of the Forest-Fire algorithm~\cite{Leskovec2007}. The algorithm has been tuned to present the following characteristics:

\begin{description}

\item[Number of states] To keep the competition setup simple enough -- by keeping two difficulty dimensions only, namely sample sparsity and alphabet size --, all state machines have approximately 50 states. Although somewhat larger than the conventional state machines identified in the Software Engineering literature, this is to ensure that any techniques submitted to this competition could scale to infer models for reasonably complex software systems. 

\item[Alphabet size] Alphabet sizes in the competition range from 2 to 50 symbols. For recall, previous competitions such as Abbadingo focussed on 2 letter alphabets. This lower bound has been kept to facilitate result comparisons. 50 symbols is representative of the upper bound found on software models extracted from the scientific litterature. 

\item[Accepting ratio] A roughly equal proportion of accepting and rejecting states has been chosen, a feature is shared with Abbadingo. While most software models, and LTS in particular, have all accepting states it has been decided to keep non accepting states as well. Doing so keeps the competition sufficiently closed to former regular induction setups, and avoids restricting it to the inference of prefix-closed regular languages in particular. As already discussed in chapter~\ref{chapter:inductive-synthesis}, infering prefix-closed languages looks an easier problem than general regular inference. Hence, a competition winner would be expected to perform equaly well, if not better, when infering machines with all accepting states.

\item[Degree distribution] While most states have in- and out-degrees of one or two transitions, state machines of software systems tend to contain a small proportion of states with a high in-degree (\emph{authority} states) or a high out-degree (\emph{hub} states). Some states are \emph{sink} accepting states, that is, they have an out-degree of 0. Hub and/or authority states, for example, typically capture specific \emph{idle} states where a software agent waits for external stimuli in terms of input events (with numerous )possible candidates). Sink states can be interpreted as explicit modeling of the ability of a system to halt, and so on. Those degree characteristics can be observed in state machines generated for the competition. It contrasts with target machines of Abbadingo where \emph{hub} or \emph{authority} states could not be present due to the small alphabet size, for example.

\item[Deterministic and minimal] Following common setup of regular inference experiments, the machines considered in the competition are both deterministic and minimal.

\end{description}

\subsection{Training and test samples}

Generating random training and testing samples for past competitions has been straightforward. The approach for competitions such as Abbadingo has been to simply generate random binary sequences up to a prescribed length, and to subsequently use the target model to classify them as ``accept'' or ``reject''. However, doing so with machines that have a larger alphabet is no longer viable; an overwhelming majority of random sequences in $\Sigma^{*}$ is likely to be classified as ``reject'', and of the few sequences that are accepted, it is unlikely that they would provide any useful coverage of the target machine. 

For this reason, training and test samples have been generated using a dedicated generation procedure. This procedure aims at simulating the way examples of system behaviour are usually obtained in the software engineering community (a collection of program traces at an implementation level or the generation of scenarios at a design level, for instance). Samples generated present the following characteristics:

\begin{description}

\item[Generated by the target] Positive strings have been generated by randomly walking the target machines. A dedicated algorithm generates positive strings by walking through the automaton from the initial state, randomly selecting outgoing transitions with a uniform distribution. When an accepting state v is reached, the generation ends with a probability of $1.0/(1 + 2*outdegree(v))$. This procedure simulates an `'end of string'' transition from state $v$ with half the probability of an existing transition. The length distribution of the strings generated is approximately centered on $5 + depth(automaton)$, a feature shared with the Abbadingo competition.

\item[Negative strings] Negative strings are generated by randomly perturbing strings generated from the target machine. Three kinds of edit operation are considered: substituting, inserting, or deleting a symbol. The editing position is randomly chosen according to a uniform distribution over the string length. Substitution and insertion also use a uniform distribution over the alphabet. The number of editing operations is chosen according to a Poisson distribution (with a mean of 3) and the three editing operations have an equal change of being selected. The randomly edited string is included in the negative sample provided it is indeed rejected by the target machine. Otherwise, it is simply discarded.

\item[Samples] The random walk algorithm and perturbation procedure serve as building blocks for the generation of training and test samples for each problem. Using them, a set of 20.000 strings has first been sampled from the target machine. This sample contains roughly equal number of positive and negative strings and usually contains duplicates. The distinct strings of the initial sample are then equally partitioned into two sets, taking care of respecting the positive and negative balance in each one. The first set is used to generate the test sample, the second one to generate the learning sample. The test sample contains 1,500 strings randomly drawn without replacement from the first set. The test set neither contains duplicates (to avoid favoring repeated strings in the scoring metric), nor intersects with the training set. The official training sets are sampled from the second set with different levels of sparsity (100\%, 50\%, 25\%, 12.5\%) and usually contain duplicates, as a consequence of the random walk generation from the target machine. As a consequence of this procedure, training and test sets do not intersect.

\item[Empirically adjusted] the different parameters above (20,000 strings initially generated, 1,500 strings maximum in the test set and the learning sample size) have all been empirically adjusted to ensure good induction results using Blue-Fringe on the simplest problems (alphabet of size 2 with a learning sample sparsity of 100\%) without breaking the cell.

\end{description}

\subsection{Submission and Scoring}

Solutions are submitted as a binary string to the competition server. For this, the competitor is expected to produce a binary sequence of labels where, for each test string, a 1 is added to the sequence if the string is considered to be accepted, and a 0 otherwise. To establish the accuracy, the sequence is compared to a reference string representing the correct classifications of the test set on the target model. The overlap between the two binary strings is measured with the \emph{Balanced Classification Rate (BCR)}. The Harmonic BCR measure is chosen because it places an equal emphasis on the accuracy of an inferred model in terms of its acceptance of positive sequences, as well as its rejection of sequences that should be rejected. It also does not require the test set to be balanced in terms of its positive / negative sequences~\cite{Walkinshaw2008}. Relaxing such assumptions is important here because the natural positive/negative balancing commonly induced by a binary alphabet disappears with larger alphabet sizes.

Harmonic BCR combines two factors. $Sensitivity$ is the proportion of positive matches that are predicted to be positive. So in terms of the sets true positives ($TP$) and false positives ($FP$), $Sensitivity=\frac{|TP|}{|TP \cup FN|}$. $Specificity$ is the proportion of true negatives that are predicted to be negative, so $Specificity=\frac{|TN|}{|TN \cup FP|}$. Harmonic BCR is the harmonic mean of the two: $$BCR=\frac{2*Sensitivity*Specificity}{Sensitivity + Specificity}$$. Note that conventionally, BCR is simply computed as the arithmetic mean of sensitivity and specificity, but the harmonic mean is preferred here because it favours balance between the two.

To avoid hill-climbing -- where the competition server could be used as an oracle to help the learner to interactively optimize a first solution --, the competition server does not respond to a submission with the exact BCR score that it has obtained. Instead, it gives a binary feedback according to whether the corresponding problem is considered broken or not. A problem is considered broken if the BCR value is greater than or equal to 0.99. A cell is considered broken if all of its 5 problems are broken. A section dedicated to each participant has been implemented on the website to provide visual feedback about the performance of their algorithm(s). In this section, problems and cells of a personal problem grid turn to green when broken.

\section{Performance of RPNI and Blue-Fringe\label{subsection_stamina_baseline}}



\section{Main results\label{subsection_stamina_results}}



