\chapter{Toward an Evaluation Platform for Inductive Model Synthesis\label{chapter:stamina}}

A grammar induction algorithm is commonly evaluated by reporting an accuracy measure of the model it learns for increasing sizes of the training sample. Plots in the previous chapter illustrate the convergence of QSM and ASM toward a ``good'' model when their input becomes rich enough. This kind of setup can also be used to make different induction techniques compete by evaluating them on common benchmarks of increasing difficulty. In that respect, the Abbadingo contest have had a notable impact~\cite{Lang:1998}. Among others, the competition triggered the discovery of the Blue-Fringe heuristics used by QSM for selecting state pairs to merge (see Section~\ref{BlueFringe}). Since its end, Abbadingo has been frequently used as a reusable benchmark and evaluation protocol for grammar induction (see e.g.~\cite{Lucas:2003, Bongard:2005, Lucas:2005, Adriaans:2006, Dupont:2008, Lambeau:2008, Heule:2010}).

A weakness of the Abbadingo benchmark, however, is that it contains induction problems for automata defined on alphabets of two letters only. Freezing certain parameters, here the alphabet size, is necessary for keeping a competition accessible to the participants. A side effect is that it limits the generalization of the conclusions as well as the re-usability of the competition protocol. 

This effect has been observed when conducting evaluations reported in this thesis. On one side, reusing Abbadingo allows comparing results on a sound and agreed benchmarking protocol. On the other side, limiting evaluations to binary alphabets is arguable from a software engineering point of view. Indeed, behavior models are commonly defined on thirty events or more. One could reasonably question the representativeness of synthetic machines used in the previous chapter for capturing behavior models.

Extending the Abbadingo protocol to take larger alphabets into account appears inappropriate. Indeed, in Abbadingo the generation of learning samples is independent from the target automaton by design. This framework inspired from probably approximately correct learning (PAC learning) \cite{Valiant:1984}, does not accurately reflect the way samples are obtained from end-user scenarios or automated testing of distributed systems. In particular, generating strings with a distribution independent of the state machine would lead to an overhelming amount of negative scenarios for typical behavior models. This is further discussed in Section~\ref{section:stamina-abbadingo}.

For this reason, we propose an alternative benchmark, called Stamina (for \emph{State Machine Inference Approaches}), which inspires from Abbadingo but focuses on the complexity of the learning with respect to the alphabet size. Among others, our protocol relies on adapted procedures for generating automata and samples. 

Stamina has initially taken the form of an induction competition, in a similar spirit to Abbadingo. The competition has officially ran between March and December 2010 and was organized in collaboration with the universities of Sheffield and Leicester; it has been officially sponsored by the Engineering and Physical Sciences Research Council\footnote{http://www.epsrc.ac.uk/}. The detailed setup of Stamina is explained in section~\ref{section:stamina-setup}. 

The winning algorithm DFASAT as well as a few additional competitors have significantly outperformed the Blue-fringe baseline on a variety of problems. DFASAT mixes SAT solving and state merging and, among other ingredients, uses a new scoring heuristics that proved especially useful to infer the kind of state machines considered in the competition. Section~\ref{section:stamina-results} presents the main results of the competition and also includes a description of the winning technique. 

In addition to pushing forward the state-of-the-art of the induction problem, the competition triggered interest from at least three communities, namely Machine Learning, Software Engineering and Formal Methods. The Stamina website\footnote{http://stamina.chefbe.net/} is still available and has been updated to serve as an online benchmark and evaluation platform for model synthesis instead of a formal competition, as explained in Section~\ref{section:stamina-platform}.

\input{src/6-stamina/1-the-abbadingo-benchmark}
\input{src/6-stamina/2-stamina-setup}
\input{src/6-stamina/3-competition-results}
\input{src/6-stamina/4-evolution}

\section*{Summary}

This chapter presented \emph{Stamina}, an evaluation platform for inductive model synthesis that completes a previous platform from the machine learning community, known as Abbadingo. Key differences with the latter is that Stamina focuses on the difficulty of the learning with respect to the alphabet size and relies on adapted protocols for generating target machines and samples presenting characteristics observed on models used by the software engineering community. Like Abbadingo, Stamina was first designed to be a formal competition in automaton induction. This competition has been won by DFASAT, a novel induction algorithm that significantly pushed forward the state-of-the-art, by outperforming Blue-fringe used as baseline. Among other ingredients, DFASAT identified an alternative scoring heuristics to the one of Blue-fringe that proved very useful for inferring the kind of behavior models proposed. Since the end of the competition, Stamina has evolved to an online benchmark along a certain number of directions. It is available at \verb|http://stamina.chefbe.net|
