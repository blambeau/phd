\section{LTS synthesis from high-level MSCs\label{section:inductive-from-hMSC}}

The previous sections have shown how system behaviors specified in collections of MSC scenarios can be first generalized as a System LTS, then decomposed as a set of agent LTSs. The technique supports the incremental enrichment of an initial scenario collection through scenario queries. It also takes other models into account, such as goals, so as to preserve multi-view consistency. Behavior generalization, incremental synthesis and multi-view consistency are the three main requirements identified in Section \ref{subsection:inductive-synthesis-requirements}. 

Coupled with other synthesis techniques such as goal mining from scenarios\footnote{whose simplest form consists in asking ``why'' when facing with a negative scenario.} \cite{Damas:2006}, interactive LTS induction is really effective; starting from a small initial scenario collection, richer system models can be synthesized in only a few iterations. Chapters~\ref{chapter:evaluation} and \ref{chapter:tool-support} illustrate this claim with evaluations and overview of the tool support.

However, for any non-toy system, a large scenario collection might become unmanageable. Among others, consistency of the collection might be difficult to guarantee without costly refactoring on scenarios. One reason is that all scenarios of a collection are required to start in the same system state; this usually implies a lot of redundancy in system descriptions.

One way to tackle this problem is to use high-level message sequence charts (hMSCs) for structuring scenario descriptions. As detailed in Section \ref{subsection:background-hmsc}, hMSCs are directed graphs where each node refers to a MSC or a finer grained hMSC (see Fig.~\ref{image:train-hmsc}). Scenarios can then be structured by introducing alternatives, sequences and loops.

Having a structured form of scenario \emph{helps} specifying a large system with scenarios; it does not \emph{solves} the problem of achieving a complete and consistent view of agent behaviors:
\begin{itemize}
\item capturing all possible interleavings of a distributed system renders difficult with scenarios; a hMSC is therefore hardly complete in practice,
\item complementary features of a system deserve being specified in complementary models; in addition to using multiple system views, having system behaviors specified in more than one hMSC makes sense.
\end{itemize}

Having a synthesis technique to merge and generalize behaviors described in hMSCs seems a convenient extension to the synthesis technique described so far. For this, the LTS synthesis statement is first revisited in Section~\ref{subsection:hmsc-induction-problem-revisited}. The inductive algorithm is then adapted in Section \ref{subsection:hmsc-induction-algo-adaptation}.

\subsection{Revisiting the LTS synthesis statement\label{subsection:hmsc-induction-problem-revisited}}

Merging multiple hMSCs $H_1,\ldots,H_n$ with respect to trace behaviors simply amounts to compute the union of their respective languages. Under all hMSC semantics considered in Section \ref{subsection:background-hmsc} (we recall them below), this is equivalent to building a new hMSC reaching the finer-grained $H_1,\ldots,H_n$ from its initial state. Additional positive scenarios, typically coming from scenario queries, could be integrated in a similar way.

TODO: an illustration is probably worth considering here?

However, a hMSC only permits specifying positive behaviors. As explained in previous sections, negative information is needed to avoid poor generalization. Negative scenarios, fluents, goals, etc. all provide a source of negative knowledge that can be used to constraint the induction process. The algorithmic adaptations considered later stay compatible with the constraint mechanism based on equivalence relations on system states.

Without much loss of generality therefore, we will assume that behaviors are specified through one hMSC only, complemented with a scenario collection. The latter contains negative scenarios and answers to scenario queries. Under these assumptions, the LTS synthesis statement is restated as follows: 

\begin{quote}
\underline{Given}~a hMSC $H$ and a scenario collection $Sc = (S^+,S^-)$ consistent with each other
\begin{align*}
[\mathcal{L}^+(Sc) \cup \mathcal{L}(H)] \cap \mathcal{L}^-(Sc) &= \emptyset
\end{align*}
\underline{Synthesize}~the system as a composition of agent LTSs
\begin{align*}
System = (Ag_1 \parallel \ldots \parallel Ag_n)
\end{align*}
\underline{Such that}~$H$, $Sc$ and $System$ are consistent.
\end{quote}

The hMSC trace semantics is left open in the formulation above. In other words, one has to decide which set of behaviors does $\mathcal{L}(H)$ denote. We recall below the relations between the three hMSC languages covered in Section \ref{subsection:background-hmsc}: 
\begin{align}
\mathcal{L}_{strong}(H) \subseteq \mathcal{L}_{weak}(H) &\subseteq \mathcal{L}_{arch}(H)
\end{align}

For recall $\mathcal{L}_{strong}(H)$ denotes the set of system behaviors with strong sequential composition of hMSC nodes and total event ordering inside MSCs. It is the simplest and most intuitive model for stakeholders involved in early phases of system design. However, it supposes an implicit synchronization scheme used by the agents that is usually not available in real distributed systems. $\mathcal{L}_{arch}(H)$ is the most realistic for such systems, as it captures all possible interleavings of agent behaviors. $\mathcal{L}_{weak}(H)$ is only used for explaining and detecting implied scenarios in hMSC specifications; it is also the hardest to compute.

Making a choice of semantics is required for generalizing behaviors because inductive synthesis takes a set of traces as input. The chosen semantics must of course fit domain assumptions. From an algorithmic point of view however, the three hMSC languages above require the same adaptations of the inductive process, as explained in the next section.

\subsection{Algorithmic adaptations\label{subsection:hmsc-induction-algo-adaptation}}

The modifications required to RPNI or QSM to adapt to this new problem statement are limited; it might look surprising at first glance.

Recall that learning a regular language $L$ aims at generalizing a positive sample $S_+$ under the control of a negative sample $S_-$ such that the following relation of language inclusions holds:
\begin{align}
S_+~~\subseteq~~L~~\subseteq~~\Sigma^*\setminus S_-
\end{align}

LTS synthesis from a scenario collection $Sc$ reduces to grammar induction because the sets $\mathcal{L}^+(Sc)$ and $\mathcal{L}^-(Sc)$ are valid positive and negative samples, respectively (see Section \ref{subsection:inductive-lts-synthesis-reduction}). In particular, they denote finite sets of traces.

When considering the generalization of hMSC behaviors, the sets of positive and negative traces are $\mathcal{L}^+(Sc) \cup \mathcal{L}(H)$ and $\mathcal{L}^-(Sc)$, respectively. The positive set is no longer a sample because $\mathcal{L}(H)$ might contain an infinite number of traces; this is true whatever hMSC semantics is chosen. Therefore, the current problem statement no longer fits exactly in the identification in the limit framework presented in Section \ref{section:inductive-background}.

In practice, it means that the positive sample can no longer be captured through a PTA; it can still be captured through a DFA, however. Required algorithmic adaptations are identified by tracking to what extent QSM relies on a tree-shaped initial solution:

\begin{description}

\item[Initialize] This function can be simply adapted to return an augmented DFA instead of a PTA. On one side, the positive traces from the hMSC may be captured through a LTS as discussed in Section \ref{subsection:background-hmsc}. On the other side, the scenario collection can be captured through an augmented PTA. These two automata can be merged through standard algorithms for capturing the union of regular languages \cite{Hopcroft:1979}. A straightforward adaptation is however required to handle error states. 

\item[ChooseStatePair] In order to preserve the merging order used by RPNI, this function must be slightly adapted. The idea is to pre-compute the natural order among the states of the initial DFA solution. A breadth first search is used and each of them is numbered when encountered. 

Not that the Blue-Fringe strategy does not require special support. The distinction between red and blue states does not depend on any tree-related assumption; in particular, the fact that a fringe state is the root of a tree is incidental (see \texttt{Merge} below).

\item[Merge] The merging for determinization process is often implemented assuming a tree invariant property. This property states that, when considering two states to be merged, at least one of them is the root of a tree. Such a property holds for RPNI and QSM, even when the Blue-Fringe optimization is used. It is a sufficient condition for the determinization process to be finite. 

Even though it is convenient, the tree invariant property is not required, as explained in \cite{Lambeau:2008}. The main merging loop and the \texttt{Merge} function can be implemented without the tree invariant property because the recursive determinization process stops naturally on the first DFA encountered. This observation allows one to start from an arbitrary DFA and, as soon as non-determinism occurs, to reduce it. Fig.~\ref{figure:merging-for-determ-on-dfa} gives an example of such a recursive operation.

\item[GenerateQuery] The generation of scenario queries relies on the tree invariant property mentioned above. When merging a state pair $(q,q')$, a scenario query is built with the shortest trace leading to $q$ concatenated with the suffixes of $q'$. When $q'$ is the root of a tree, generating a finite scenario is straightforward.

If the tree invariant property no longer holds, the \texttt{GenerateQuery} function must be extended with a procedure for extracting finite suffixes from $q'$. This does not introduce any technical issues; for example, pre-computing a spanning tree on the initial DFA would associate finite suffixes to each of its state. However, what forms a ``good'' suffix for convergence and scenario classification by end-users is an open question. As the adapted algorithm no longer fits in the identification in the limit framework, the notion of a characteristic sample would need to be adapted. 

\end{description}

\begin{figure}\centering
\scalebox{.34}{
\includegraphics*{src/4-inductive/images/merging-for-determ-on-dfa}}
\caption{Recursive determinization process. States \{3\} and \{0\} of an arbitrary DFA are merged, which causes a non-determinism on letter $b$ from state \{0,3\}. The destination states \{2\} and \{4\} are subsequently merged to reduce the non-determinism.\label{figure:merging-for-determ-on-dfa}} 
\end{figure}
  
