\section{Summary and discussion\label{section:inductive-discussion}}

This chapter discussed how grammar induction can be used to synthesize LTS state machines from end-user scenarios. The RPNI algorithm provides a basis to generalize system behaviors described in the scenarios as a system LTS; the latter can then be decomposed to obtain a state machine for each system agent. 

Our QSM algorithm extends RPNI with an interactive feature where an end-user classifies generated scenarios as positive or negative examples of desired system behaviors. This constrains the induction process towards good behavior generalizations and completes the initial scenario collection with additional behavior examples and counterexamples.

Moreover, QSM may be constrained through equivalence relations defined on system states. This mechanism has been instantiated with fluent definitions, domain properties, models of legacy components and goals. In addition to guaranteeing multi-view model consistency, the injection of such knowledge prunes the induction search space for better performance.



\subsection{Rationale behind the generalization step}

Our synthesis requirements include ``behavior generalization''. Looking at relation (\ref{relation:inductive-language-refinements}) we might ask ourselves what drives the generalization process and until when. By construction, the first automaton $A_0$ already meets the consistent system view conditions (\ref{relation:inductive-invariant}) and (\ref{relation:inductive-invariant-II}). Therefore, $A_0$ is a valid, yet trivial, solution. Why not simply use it?

The answer is to be found in the Occam's principle stating that ``among all models explaining the world equally well, the simplest should be preferred''. Using grammar induction this amounts to searching for the \emph{smallest} automaton consistent with the positive and negative scenarios, also called the \emph{input sample}. The initial automaton $A_0$ is rarely the simplest model according to this criteria. 

Looking for the smallest automaton consistent with an input sample is known to be NP-hard \cite{Gold:1978, Angluin:1978}. The RPNI algorithm offers a consistent approximated solution in polynomial time; this solution is the smallest consistent deterministic automaton when the input sample is rich enough, in particular when it forms a so-called \emph{characteristic sample} (see later) \cite{Oncina:1992}.

\subsection{Correctness}

The decomposition step guarantees that the \emph{structural consistency} and \emph{consistent agent view} conditions hold. It is a straight application of the material given in Chapter~\ref{chapter:framework}. 

The generalization invariant guarantees that the \emph{consistent system view} condition holds for the System LTS. Strictly speaking, the approach is correct only if this condition holds for the system re-composition $\system$. This might not be the case in presence of negative implied scenarios.

\subsection{Future directions}

From a grammar induction point of view, the ASM algorithm can be seen as generalizing any positive regular language $\mathcal{L}^+$ under the control of a negative sample $S^-$. As such, RPNI is thus a special case where the positive language forms a sample $S^+$, that is a finite set of strings.

Moreover, goals and domain properties can still be used to prune the ASM search space with the technique discussed in Section~\ref{subsection:induction-pruning-with-goals}. As goals actually capture negative languages through their tester automaton, this amounts to consider a generalization of ASM to generalize a positive language $\mathcal{L}^+$ under the control of a negative one $\mathcal{L}^-$. This generalization is called ASM$^*$ and briefly discussed in \cite{Lambeau:2008}.
