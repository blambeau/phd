\section{Discussion\label{section:inductive-discussion}}

Let us step back a little before closing this chapter. This discussion section aims at answering two questions:
\begin{itemize}
\item \emph{What problem are we trying to solve?}
\item \emph{Why do we solve it in this way?}
\end{itemize}
In addition to revisiting our synthesis technique and discussing some perspectives, answers to those questions will allow us to compare our approach with alternative ones. Such comparisons are conducted in Chapter~\ref{chapter:related-work}. 

Let us start with the former question, ``what problem are we trying to solve?''. 

This chapter discussed an horizontal synthesis technique. Remember from Section~\ref{section:intro-synthesis} that the underlying objective is to elaborate requirements and explore system designs. In this context, horizontal synthesis aims at building models missing from a multi-view framework or completing existing ones.

In the light of this objective, our inductive synthesis technique can be seen as a building block in the tackling of a larger multi-view modeling problem. This problem can be stated as follows:
\begin{itemize}
\item There is an expected target system, composed of agents $Ag_1, \ldots, Ag_n$ whose behavior can be modeled through state machines. The behavior of the system itself is defined through agent composition.
\item A complete description of system behaviors is not available. Behavioral model \emph{fragments} may however be available; those fragments take various forms:
\begin{itemize}
\item Scenarios may describe examples and counterexamples of desired system behaviors.
\item The behavior of some agents may be partially or completely captured through definitions of agent state variables and/or known state machines.
\item Known behavioral goals may define restrictions on agent and system behaviors so as to avoid undesired system histories.
\end{itemize}
\item System behaviors must be specified more completely. This roughly means completing all models in an incremental, consistent and (hopefully) convergent way.
\end{itemize}

As just stated, solving the problem above presupposes an incremental approach. Within iterations, system descriptions gets more complete and behavior models gets richer. Additional system features are modeled when successful iterations have completed; refactoring steps are conducted when problems appear, such as undesired implied scenarios.

Our inductive state machine synthesis technique is only a building block in an integrated approach along this line\footnote{The ISIS tool introduced in Section~\ref{section:tool-support-isis} goes one step further as it integrates QSM with other synthesis techniques, such as the inference of behavioral goals from scenarios.}. Observe that the synthesis of state machines from scenarios can be partially seen as a pretext. In the rather opinionated multi-view vision above, state machine synthesis is seen as equally important as the completion of the scenario collection with answers to scenario questions; the same applies to the identification of behavior goals triggered when some of these scenarios are rejected.

The following sections revisit and discuss design choices of our synthesis approach in the light of the multi-view modeling vision above.

\subsection{Agent behaviors, or system behaviors?}

The choice of tackling the synthesis of \emph{agent} state machines though the \emph{system} behaviors has not been motivated. Why not inferring one state machine by agent and composing them afterwards, instead of going the other way round?

First, let us stress that such alternative approach makes perfect sense. Our experience in exploring it even suggests that it may provided good results in terms of behavior generalization. That said, a few points would need to be adapted and/or taken into account to further explore it.
\begin{itemize}
\item Scenario questions would not be available for guiding induction in a per-agent approach. The interactive feature could be adapted but that would require the user to be able to classify agent traces. This seems less convenient as it amounts to interact with the end-user in a different language for each agent. Moreover, interactions would not occur in the language used by the end-user for specifying scenarios in the first place. 
\item Our definition of negative scenarios is borrowed from \cite{Uchitel:2002}. Such definition amounts to consider negative \emph{system} traces only. For an effective use of negative scenarios for locally inferring agent state machines, this definition would need to be adapted.
\item Similarly, our approach considers goals and domain properties system-wide. Unless denoting agent requirements specifically, known safety properties could not be used to constrain the induction process. Even in the absence of implied scenarios, the consistency of inferred state machines with goals would be harder to guarantee.
\end{itemize}

\subsection{The rationale behind grammar induction}

From the specific standpoint of state machine synthesis, our use of grammar induction is motivated by the need of generalizing system behaviors described in the scenarios. This choice is further discussed here in the broader context of the incremental elaboration of requirements driven by the multi-view building of behavior models.

To keep things simple enough in the sequel, we leave aside our use of fluent state variables, models of legacy components, the structure of scenarios, the decomposition step of our approach, and implied scenarios. In other words, we focus here on pure behavioral aspects in the triangle composed of scenarios, state machines and goals \cite{Damas:2006, Uchitel:2007}.
\begin{itemize}
\item On one side, positive scenarios capture a lower bound on system behaviors. Scenarios capture behaviors that the system \emph{must} exhibit. The enrichment of the scenario language in the transition from QSM to ASM amounts capturing these positive behaviors through a (prefix-closed) regular language, in contrast to a sample made of a finite number of traces. Let denote this language by $\mathcal{L}^+(\me{Scenarios})$. 
\item On the other side, negative scenarios and goals capture an upper bound on system behaviors. They capture behaviors that the system \emph{may not} exhibit. The system traces violating safety properties are known to be captured by the class of regular languages (see Section~\ref{subsection:background-property-and-tester-automata}). Negative scenarios generally illustrate violations of safety properties; we may thus ignore them without loss of generality. Let denote undesired behaviors by $\mathcal{L}^-(\me{Goals})$.
\item Somewhere between these bounds, agent state machines may be seen as capturing the exact behaviors exhibited by the system. Provided those state machines are known, $\mathcal{L}(\agentscomposed)$ denotes the system behaviors. 
\end{itemize}

The consistency of scenarios, state machines and goals is captured by the following conditions.
\begin{align*}
&\mathcal{L}^+(\me{Scenarios}) \subseteq \mathcal{L}(\agentscomposed) \\
&\mathcal{L}^-(\me{Goals}) \cap \mathcal{L}(\agentscomposed) = \emptyset
\end{align*}

As stated in Section~\ref{section:intro-synthesis}, an horizontal synthesis technique typically uses precise consistency rules backwards. Instead of checking them, horizontal synthesis uses these rules to semi-automatically synthesize model fragments using the information already available in other views. Our inductive state machine synthesis technique is illustrative of such process. It can be seen as synthesizing missing state machines from scenarios, under the control of goals:
\begin{align}
&\mathcal{L}^+(\me{Scenarios}) \subseteq \mathcal{L}(?) \subseteq  \Sigma^* \setminus \mathcal{L}^-(\me{Goals})
\end{align}

The choice of grammar induction methods to tackle the synthesis problem is not incidental. More than offering a practical way of generalizing system behaviors illustrated in the scenarios, it provides convergence guarantees towards the target system when the scenarios and goals get rich enough. Moreover, such convergence criterion looks necessary even if state machine synthesis is used as a way of elaborating requirements or completing a multi-view system model\footnote{It should be noted here, though, that discussing the convergence of such elaboration process only makes sense under the assumption that the expected target system exists and is fixed, even if unknown.}.


%\subsection{Rationale behind the generalization step}

%Our synthesis requirements include ``behavior generalization''. Looking at relation (\ref{relation:inductive-language-refinements}) we might ask ourselves what drives the generalization process and until when. By construction, the first automaton $A_0$ already meets the consistent system view conditions (\ref{relation:inductive-invariant}) and (\ref{relation:inductive-invariant-II}). Therefore, $A_0$ is a valid, yet trivial, solution. Why not simply use it?

%The answer is to be found in the Occam's principle stating that ``among all models explaining the world equally well, the simplest should be preferred''. Using grammar induction this amounts to searching for the \emph{smallest} automaton consistent with the positive and negative scenarios, also called the \emph{input sample}. The initial automaton $A_0$ is rarely the simplest model according to this criteria. 

%Looking for the smallest automaton consistent with an input sample is known to be NP-hard \cite{Gold:1978, Angluin:1978}. The RPNI algorithm offers a consistent approximated solution in polynomial time; this solution is the smallest consistent deterministic automaton when the input sample is rich enough, in particular when it forms a so-called \emph{characteristic sample} (see later) \cite{Oncina:1992}.

%\subsection{Future directions}

%From a grammar induction point of view, the ASM algorithm can be seen as generalizing any positive regular language $\mathcal{L}^+$ under the control of a negative sample $S^-$. As such, RPNI is thus a special case where the positive language forms a sample $S^+$, that is a finite set of strings.

%Moreover, goals and domain properties can still be used to prune the ASM search space with the technique discussed in Section~\ref{subsection:induction-pruning-with-goals}. As goals actually capture negative languages through their tester automaton, this amounts to consider a generalization of ASM to generalize a positive language $\mathcal{L}^+$ under the control of a negative one $\mathcal{L}^-$. This generalization is called ASM$^*$ and briefly discussed in \cite{Lambeau:2008}.
