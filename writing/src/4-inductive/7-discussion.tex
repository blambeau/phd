\section{Discussion\label{section:inductive-discussion}}

Let us step back a little before closing this chapter. This discussion section aims at answering two questions:
\begin{itemize}
\item \emph{What problem are we trying to solve?}
\item \emph{Why do we solve it in this way?}
\end{itemize}
In addition to revisiting our synthesis technique and discussing some perspectives, answers to those questions will allow us to compare our approach with alternative ones. Such comparisons are conducted in Chapter~\ref{chapter:related-work}. 

Let us start with the former question, ``what problem are we trying to solve?''. 

This chapter discussed an horizontal synthesis technique. Remember from Section~\ref{section:intro-synthesis} that the underlying objective is to elaborate requirements and explore system designs. In this context, horizontal synthesis aims at building models missing from a multi-view framework or completing existing ones.

In the light of this objective, our inductive synthesis technique can be seen as a building block in the tackling of a larger multi-view modeling problem. This problem can be stated as follows:
\begin{itemize}
\item There is an expected target system, composed of agents $Ag_1, \ldots, Ag_n$ whose behavior can be modeled through state machines. The behavior of the system itself is defined through agent composition.
\item A complete description of system behaviors is not available. Behavioral model \emph{fragments} may however be available; those fragments take various forms:
\begin{itemize}
\item Scenarios may describe examples and counterexamples of desired system behaviors.
\item The behavior of some agents may be partially or completely captured through definitions of agent state variables and/or known state machines.
\item Known behavioral goals may define restrictions on agent and system behaviors so as to avoid undesired system histories.
\end{itemize}
\item System behaviors must be specified more completely. This roughly means completing all models in an incremental, consistent and (hopefully) convergent way.
\end{itemize}

As just stated, solving the problem above presupposes an incremental approach. Within iterations, system descriptions gets more complete and behavior models gets richer. Additional system features are modeled when successful iterations have completed; refactoring steps are conducted when problems appear, such as undesired implied scenarios.

Our inductive state machine synthesis technique is only a building block in an integrated approach along this line\footnote{The ISIS tool introduced in Section~\ref{section:tool-support-isis} goes one step further as it integrates QSM with other synthesis techniques, such as the inference of behavioral goals from scenarios.}. Observe that the synthesis of state machines from scenarios can be partially seen as a ``pretext''. In the rather opinionated multi-view vision above, state machine synthesis is seen as equally important as the completion of the scenario collection with answers to scenario questions; the same applies to the identification of behavior goals triggered when some of these scenarios are rejected.

The following sections revisit and discuss design choices of our synthesis approach in the light of the multi-view modeling vision above.

\subsection{Agent behaviors, or system behaviors?}

The choice of tackling the synthesis of \emph{agent} state machines though the \emph{system} behaviors has not been motivated. Why not inferring one state machine by agent and composing them afterwards, instead of going the other way round?

First, let us stress that such alternative approach makes perfect sense. Our experience in exploring it even suggests that it may provided good results in terms of behavior generalization. That said, a few points would need to be adapted and/or taken into account to further explore it.
\begin{itemize}
\item Scenario questions would not be available for guiding induction in a per-agent approach. The interactive feature could be adapted but that would require the user to be able to classify agent traces. This seems less convenient as it amounts to interact with the end-user in a different language for each agent. Moreover, interactions would not occur in the language used by the end-user for specifying scenarios in the first place. 
\item Our definition of negative scenarios is borrowed from \cite{Uchitel:2002}. Such definition amounts to consider negative \emph{system} traces only. For an effective use of negative scenarios for locally inferring agent state machines, this definition would need to be adapted.
\item Similarly, our approach considers goals and domain properties system-wide. Unless denoting agent requirements specifically, known safety properties could not be used to constrain the induction process. Even in the absence of implied scenarios, the consistency of inferred state machines with goals would be harder to guarantee.
\end{itemize}

\subsection{The rationale behind grammar induction}

From the specific standpoint of state machine synthesis, our use of grammar induction was motivated by the need of generalizing system behaviors described in the scenarios. This choice is further discussed here in the broader context of the incremental elaboration of requirements driven by the multi-view building of behavior models.

To keep things simple enough in the sequel, we leave aside structural system aspects hidden behind fluents state variables, our use of legacy components, the structure of scenarios, the decomposition step of our approach, and implied scenarios. In other words, we focus here on pure behavioral aspects in the triangle composed of scenarios, state machines and goals \cite{Damas:2006, Uchitel:2007}.
\begin{itemize}
\item On one side, positive scenarios capture a lower bound on system behaviors. In other words, scenarios denote behaviors that the system \emph{must} exhibit. Let denote these behaviors by $\mathcal{L}^+(\me{Scenarios})$. The enrichment of the scenario language in the transition from QSM to ASM amounts considering that positive system behaviors are captured by an arbitrary (prefix-closed) regular language.
\item On the other side, negative scenarios and goals capture an upper bound on system behaviors, that is, they capture behaviors that the system \emph{may not} exhibit. As negative scenarios illustrate violations of safety properties, we may ignore them without loss of generality. Let therefore denote these undesired behaviors by $\mathcal{L}^-(\me{Goals})$. In our formal framework, this language may also denote an arbitrary regular language.
\item Between them, 
\end{itemize}

The enrichment of the scenario language in the transition from QSM to ASM allows us to consider that the behaviors described in scenarios are entirely specified 

\subsection{Why do we solve it in this way?}

Some standpoints in the problem formulation above are debatable. One could ask whether this incremental process has an end or argue that the ultimate goal is the elaboration of requirements in particular. 

Whatever the answers, we state two reasonable expectations on approaches supporting the incremental building of a multi-view behavior model:
\begin{quotation}
\emph{Every time a successful iteration completes, the available models should provide a consistent view of system behaviors.}
\end{quotation}
\begin{quotation}
\emph{If failing to complete, the elaboration process should at least provide a reasonable convergence criteria.}
\end{quotation}

Those left aside, the problem stated in previous section can be stated precisely as follows \cite{Uchitel:2007}:
\begin{itemize}
\item There is an expected target system whose behaviors can be captured by a composition of agent state machines $\mathcal{L}(\me{System}) = \mathcal{L}(\agentscomposed)$.
\item Positive scenarios, either in a structured or non structured form, capture a lower bound on system behaviors. In other words, scenarios denote behaviors that the system \emph{must} exhibit. Let denote these behaviors by $\mathcal{L}^+(\me{Scenarios})$.
\item Negative scenarios and goals capture an upper bound on system behaviors, that is, they capture behaviors that the system \emph{may not} exhibit. As negative scenarios generally illustrate the violation of goals, we may ignore them here without loss of generality. Let therefore denote these behaviors by $\mathcal{L}^-(\me{Goals})$.
\item We are interested in specifying the system behaviors more completely. This roughly means completing available scenarios, state machines and goals in a consistent way.
\end{itemize}

The two expectations on the elaboration process above can now be stated precisely as follows:
\begin{itemize}
\item Every time a successful iteration completes, scenarios, state machines and goals should provide a consistent view of system behaviors:
\begin{align}
&\mathcal{L}^+(\me{Scenarios}) \subseteq \mathcal{L}(\me{System})\label{elaboration-process-consistency-1}\\
&\mathcal{L}^-(\me{Goals}) \cap \mathcal{L}(\me{System}) = \emptyset\label{elaboration-process-consistency-2}
\end{align}
\item If failing to complete, the elaboration process should at least provide a reasonable convergence criteria towards a situation where scenarios, state machines and goals describe the same system:
\begin{align}
\mathcal{L}^+(\me{Scenarios})~=~\mathcal{L}(\me{System})~=~\Sigma^* \setminus \mathcal{L}^-(\me{Goals})
\end{align}
\end{itemize}

The synthesis techniques discussed in this chapter do not provide a full solution to this general problem. However, we argue that they provide a \emph{sound} way of tackling part of the problem, namely the synthesis of state machines consistent with available scenarios and goals. Interestingly enough, the completion of scenarios and identification of missing goals triggers subsequent iterations of the elaboration process.

The soundness of our approach mostly results from its grammar induction background.
\begin{description}
\item[Consistency] Conditions (\ref{elaboration-process-consistency-1}) and (\ref{elaboration-process-consistency-2}) are slight generalizations of the ``consistent system view'' conditions discussed in depth in this chapter (see Sections~\ref{subsection:inductive-synthesis-statement} and \ref{section:inductive-correctness} in particular). They can be rewritten as follows:
\begin{align}
\mathcal{L}^+(\me{Scenarios})~\subseteq~\mathcal{L}(\me{System})~\subseteq~\Sigma^* \setminus \mathcal{L}^-(\me{Goals})
\end{align}

From the standpoint of grammar induction, the system synthesis problem therefore amounts to consider the generalization of the positive scenario language under the control of a negative one given by negative scenarios and goals. 

Restricting our attention to safety properties only but allowing structured form of scenarios, these two languages may denote arbitrary regular languages. 

\item[Convergence]
\end{description}



%\subsection{Rationale behind the generalization step}

%Our synthesis requirements include ``behavior generalization''. Looking at relation (\ref{relation:inductive-language-refinements}) we might ask ourselves what drives the generalization process and until when. By construction, the first automaton $A_0$ already meets the consistent system view conditions (\ref{relation:inductive-invariant}) and (\ref{relation:inductive-invariant-II}). Therefore, $A_0$ is a valid, yet trivial, solution. Why not simply use it?

%The answer is to be found in the Occam's principle stating that ``among all models explaining the world equally well, the simplest should be preferred''. Using grammar induction this amounts to searching for the \emph{smallest} automaton consistent with the positive and negative scenarios, also called the \emph{input sample}. The initial automaton $A_0$ is rarely the simplest model according to this criteria. 

%Looking for the smallest automaton consistent with an input sample is known to be NP-hard \cite{Gold:1978, Angluin:1978}. The RPNI algorithm offers a consistent approximated solution in polynomial time; this solution is the smallest consistent deterministic automaton when the input sample is rich enough, in particular when it forms a so-called \emph{characteristic sample} (see later) \cite{Oncina:1992}.

%\subsection{Future directions}

%From a grammar induction point of view, the ASM algorithm can be seen as generalizing any positive regular language $\mathcal{L}^+$ under the control of a negative sample $S^-$. As such, RPNI is thus a special case where the positive language forms a sample $S^+$, that is a finite set of strings.

%Moreover, goals and domain properties can still be used to prune the ASM search space with the technique discussed in Section~\ref{subsection:induction-pruning-with-goals}. As goals actually capture negative languages through their tester automaton, this amounts to consider a generalization of ASM to generalize a positive language $\mathcal{L}^+$ under the control of a negative one $\mathcal{L}^-$. This generalization is called ASM$^*$ and briefly discussed in \cite{Lambeau:2008}.
