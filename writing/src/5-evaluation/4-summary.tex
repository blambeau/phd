\section{Summary and discussion\label{section:evaluation-summary}}

This chapter discussed the evaluation of the inductive LTS synthesis technique presented in Chapter \ref{chapter:inductive-synthesis}. Evaluations have been conducted on case-studies and complemented with experiments on synthetic datasets. The latter provide an overview of what can be expected along two dimensions not completely covered with case-studies: the size of the target LTS model and the sparsity of the learning sample.  

From the point of view of multi-view model synthesis, the following conclusions can be drawn:
\begin{itemize}

\item Together with the Blue-fringe heuristics, the interactive feature of QSM is very effective for reaching a good accuracy of synthesized models. The cost in terms of scenario questions may be important, especially on large systems. Therefore, techniques to reduce the number of scenarios to be classified are required in practice.

\item Among our pruning techniques, the domain knowledge provided by fluents, goals and legacy components proves very useful. In addition to reducing the number of scenario questions, such knowledge effectively guides the induction process toward accurate LTS models. 

\item The control information given by a hMSC, simulated in evaluations through a dedicated procedure, also provides an important accuracy gain. While it guides the generalization process toward good generalizations, it has been shown that it does not provide the negative knowledge required to avoid over-generalization.

\item The most important issue with QSM is related to the number of generated scenario questions. As shown in Table~\ref{All:res} and in Fig.~\ref{image:evaluation-qsm-number-of-questions}, the number of rejected questions tend to decrease when the induction input gets richer, either because the sample gets larger or thanks to available domain knowledge. The same is not true for scenarios to be accepted, whose number gets roughly constant.

In practice, this means that QSM will generate scenarios even when its input is rich enough to guarantee very good generalizations without even asking questions (e.g., when given a characteristic sample). Moreover, fluents and goals do not help reducing these questions. As their number might become large for non-toy systems, improvements of our technique might be necessary.

One possible solution could be to extend the possible answers from the oracle. For example, the end-user could request to terminate the induction immediately, without additional scenario questions. The infered state machine would then be submitted for inspection, leading to a form of \emph{equivalence} queries. Such queries are typically answered with an additional negative scenario and the induction restarted if the state machine is not adequate (e.g. \cite{Angluin:1987}, see also Chapter~\ref{chapter:discussion}).

\end{itemize}

We close this chapter by discussing specific issues related to the experiments themselves and the protocols used to conduct them. 
\begin{itemize}

\item The coverage of conducted experiments could be slightly extended. In particular, the use of control information and ASM could be further studied with more case-studies. The real use of a hMSC instead of a simulation procedure is worth considering as well.

Our current implementation of ASM relies on the RPNI search order and does not support the interactive feature of QSM. This limited the possible comparisons during evaluations. In particular, the effect of control information on the number of generated questions has not been illustrated.

\item The experiments on synthetic datasets provide an in-depth analysis of the performances of QSM and ASM with RPNI and Blue-fringe, which have been taken as state-of-the-art induction algorithms. 

Another induction algorithm called DFASAT recently appeared that significantly outperforms RPNI and Blue-fringe \cite{Heule:2010} (see Chapter~\ref{chapter:stamina}). This opens new perspectives for comparisons, evaluations and further developments around QSM and ASM. 

\item In contrast to the smaller but ``real'' case studies, the experiments on synthetic datasets are such that target LTS are defined on alphabets of two symbols only. This is a consequence of reusing the protocol from Abbadingo for conducting experiments. 

As shown by the case studies themselves, state machines of software systems are commonly defined on larger alphabets. While this does not hurt the soundness of our experiments themselves, it slighlty prevent from generalizing their results. The next chapter addresses this issue by describing our work on the Stamina plateform for evaluating inductive model synthesis techniques with an alternative protocol to Abbadingo.
\end{itemize}
