\section{Summary and discussion\label{section:evaluation-summary}}

This chapter discussed the evaluation of the inductive LTS synthesis technique presented in Chapter \ref{chapter:inductive-synthesis}. Evaluations have been conducted on case-studies and complemented with experiments on synthetic data. From the point of view of multi-view model synthesis, the following conclusions can be drawn:
\begin{itemize}
\item Together with the Blue-fringe heuristics, the interactive feature of QSM helps reaching a good accuracy of synthesized models. The cost in terms of scenario questions may be important, especially on large systems. Therefore, techniques to reduce the number of scenarios to be classified are required in practice.
\item Among them, the domain knowledge provided by fluents, goals and legacy components proves very useful. In addition to reducing the number of scenario questions, such knowledge effectively guides the induction process toward accurate LTS models. 
\item The control information given by a hMSC, simulated in the chapter through a dedicated procedure, also provides an important accuracy gain. Such control information is shown to be complementary to the domain knowledge offered by fluents and goals. While it guides the generalization process toward good state merging, thereby avoiding poor generalizations, it does not provide the negative knowledge required to avoid over-generalization.
\end{itemize}

Experiments on synthetic data provide in-depth comparisons of the QSM and ASM algorithms with RPNI and Blue-fringe, taken as state-of-the-art induction algorithms. The evaluation protocol used for those experiments follows the one used during the well-known Abbadingo contest \cite{Lang:1998}. 

The experiments on synthetic data confirm the observations made when evaluating our techniques on case studies. In addition, they provide an overview of what can be expected along two dimensions not completely covered with case-studies: the size of the target LTS model and the sparsity of the learning sample, i.e. the size of the initial scenario collection. Those experimental results are given through plots measuring the generalization accuracy, number of scenario questions and induction time along these two dimensions.

We close this chapter by discussing two specific issues. The first one is probably the most important issue when using QSM in practice. The second one is related to the protocol used for conducting experiments on synthetic data. 

\begin{itemize}
\item When using QSM, the number of rejected scenarios decreases when the input becomes richer. This is clearly illustrated in Table~\ref{All:res} on case studies, and in Fig.~\ref{image:evaluation-qsm-number-of-questions} when the learning sample becomes larger on synthetic data. The same is not true for scenarios to be accepted, whose number tend to become constant in that case. 

In practice, this means that QSM will generate scenarios even when the input is rich enough to guarantee very good generalizations without even asking questions (e.g. a characteristic sample). Fluents, goals and control information do not help reducing these questions. As their number might become large for non-toy systems, improvements of our technique might be necessary to reduce them.

\item The protocol used in experiments on synthetic data is such that target LTS are defined on alphabets of two symbols only. As show by the case studies themselves, state machines of software systems are commonly defined on larger alphabets. 

The next chapter adresses this specific issue by describing our work on the Stamina plateform for evaluating inductive model synthesis techniques.   
\end{itemize}
