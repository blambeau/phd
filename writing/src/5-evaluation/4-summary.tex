\section{Discussion\label{section:evaluation-summary}}

This chapter discussed our evaluation of the inductive LTS synthesis technique presented in Chapter \ref{chapter:inductive-synthesis}. The evaluations were conducted on case studies complemented with experiments on synthetic datasets. For recall, with respect to the thesis objectives the main evaluation question were:

\begin{quotation}
\emph{How well does inductive synthesis help building \emph{adequate}, \emph{complete}, \emph{consistent} and \emph{precise} models for the target system considered?}
\end{quotation}

The following conclusions can be drawn.
\begin{itemize}

\item The overal effectiveness of our multi-view approach has been demonstrated on case-studies. Within a few iterations in the ISIS tool, adequate state machines are found. Scenario questions complete the scenario collection with interesting examples and counterexamples of system behavior. They also trigger the identification of important system requirements and domain properties, thereby completing the overal multi-modeling in a consistent way.

\item Together with the Blue-fringe heuristics, the interactive feature of QSM is very effective in reaching a good adequacy of synthesized models. The cost in terms of number of scenario questions may be significant, especially for large systems. Therefore, techniques for reducing the number of scenarios to be classified are required in practice.

\item Among our pruning techniques, the domain knowledge provided by fluents, goals and legacy components proves very effective. In addition to reducing the number of scenario questions, such knowledge effectively guides the induction process towards adequate LTS models. 

\item The control information provided by a hMSC, simulated in evaluations through a dedicated procedure, also results in important gain in adequacy. While guiding the generalization process towards good generalizations, it has been shown that it does not provide the negative knowledge required to avoid overgeneralization.

\item When considering model adequacy, induction time and real-time usability, the approach has been shown to scale. Experiments on synthetic data have been conducted on target machines whose size is significantly larger that the case-studies considered. A convergence towards adequate models within reasonable time bounds has been observed in all cases.

\item The most important issue with QSM is related to the number of generated scenario questions. As shown in Table~\ref{All:res} and in Fig.~\ref{image:evaluation-qsm-number-of-questions}, the number of rejected questions tends to decrease as the induction input gets richer, either because the sample gets larger or thanks to available domain knowledge. This contrasts with scenarios to be accepted, whose number tends to become constant in that case.

This means in practice that QSM will generate scenarios even when its input is rich enough to guarantee very good generalizations without even asking questions -- e.g., when given a characteristic sample. Moreover, fluents and goals do not help reducing these questions. As their number might become large for non-toy systems, improvements of our technique might be necessary for guaranteeing the scalability of the approach. 

One possible solution could be to extend the possible answers from the oracle. For example, the end-user could request the algorithm to terminate immediately, that is, without generating further scenario questions. The inferred state machine would then be submitted for inspection, leading to a form of equivalence queries. When the state machine is not adequate, such queries are typically answered with an additional negative scenario and the induction restarted -- e.g., as in L$^*$ \cite{Angluin:1987} (see also Chapter~\ref{chapter:related-work}).

\end{itemize}

Let us close this chapter with specific issues related to the experiments themselves and the protocols used to conduct them. 
\begin{itemize}

\item The coverage of conducted experiments could be slightly extended. In particular, ASM could be further studied on more case studies. The real use of a hMSC instead of a simulation procedure for control information is worth considering as well.

Moreover, our current implementation of ASM relies on the RPNI search order and does not support the interactive feature of QSM. This limited the possible comparisons during evaluations. In particular, the effect of control information on the number of generated questions has not been studied.

\item The experiments on synthetic datasets provide performance comparisons for RPNI, Blue-fringe, QSM and ASM. The former two were used as state-of-the-art induction algorithms. 

Another induction algorithm called DFASAT recently appeared that significantly outperforms RPNI and Blue-fringe \cite{Heule:2010} (see Chapter~\ref{chapter:stamina}). This opens new perspectives for comparisons, evaluations and further developments around QSM and ASM. 

\item In contrast to the smaller but ``real'' case studies, the experiments on synthetic datasets referred to target LTS defined on alphabets of two symbols only. This is a consequence of reusing the protocol from Abbadingo for conducting experiments. 

As shown in the case studies themselves, state machine models of software systems are commonly defined on much larger alphabets. While this does not hurt the soundness of our evaluations, it slightly prevent us from generalizing their results. The next chapter will address this issue by describing further work on our Stamina platform for evaluating inductive model synthesis techniques.
\end{itemize}
