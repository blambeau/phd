\section{Summary and discussion\label{section:evaluation-summary}}

This chapter discussed the evaluation of the inductive LTS synthesis technique presented in Chapter \ref{chapter:inductive-synthesis}. Evaluations have been conducted on case-studies and complemented with experiments on synthetic datasets. From the point of view of multi-view model synthesis, the following conclusions can be drawn:
\begin{itemize}
\item Together with the Blue-fringe heuristics, the interactive feature of QSM is very effective for reaching a good accuracy of synthesized models. The cost in terms of scenario questions may be important, especially on large systems. Therefore, techniques to reduce the number of scenarios to be classified are required in practice.
\item Among our pruning techniques, the domain knowledge provided by fluents, goals and legacy components proves very useful. In addition to reducing the number of scenario questions, such knowledge effectively guides the induction process toward accurate LTS models. 
\item The control information given by a hMSC, simulated in evaluations through a dedicated early merging procedure, also provides an important accuracy gain. Such control information is shown to be complementary to the domain knowledge offered by fluents and goals. While it guides the generalization process toward good generalizations, it does not provide the negative knowledge required to avoid over-generalization.
\end{itemize}

Experiments on synthetic datasets provide an in-depth analysis of the performances of QSM and ASM. For conducting comparisons, RPNI and Blue-fringe have been taken as state-of-the-art induction algorithms. 

The experiments confirm the observations made when evaluating our algorithms on case studies. In addition, they provide an overview of what can be expected along two dimensions not completely covered with case-studies: the size of the target LTS model and the sparsity of the learning sample. Those experimental results are given through plots reporting the generalization accuracy, number of scenario questions and induction time along these two dimensions.

We close this chapter by discussing two specific issues. The first one is probably the most important issue for using QSM in practice. The second one is related to the protocol used for conducting experiments on synthetic data. 

\begin{itemize}
\item When using QSM, the number of rejected scenarios decreases when the input becomes richer. This is clearly illustrated in Table~\ref{All:res} on case studies, and in Fig.~\ref{image:evaluation-qsm-number-of-questions} when the learning sample becomes larger on synthetic data. The same is not true for scenarios to be accepted, whose number tend to become constant in that case. 

In practice, this means that QSM will generate scenarios even when its input is rich enough to guarantee very good generalizations without even asking questions (e.g., when given a characteristic sample). Fluents, goals and control information do not help reducing these questions. As their number might become large for non-toy systems, improvements of our technique might be necessary.

\item The protocol used in experiments on synthetic data is such that target LTS are defined on alphabets of two symbols only. As shown by the case studies themselves, state machines of software systems are commonly defined on larger alphabets. 

The next chapter addresses this specific issue by describing our work on the Stamina plateform for evaluating inductive model synthesis techniques with an alternative protocol to Abbadingo.
\end{itemize}
